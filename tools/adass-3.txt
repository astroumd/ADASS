192 x 29 in reg/ADASS 2018  Submitted Abstracts.xls
Accepted 188 entries
8 x 29 in reg/ADASS 2018  Submitted Abstracts(1).xls
Accepted 5 entries
269 x 36 in reg/ADASS 2018  Total Registrant Re.xls
Accepted 252 entries
 
O Adámek, Karel karel.adamek@oerc.ox.ac.uk A GPU implementation of the harmonic sum algorithm
    ABS: Detecting pulsars in time-domain radio astronomy using Fourier transform based techniques is a convenient and computationally efficient way to extract the faint single pulses from the noise in which they sit. However this technique, called periodicity searching, has some pitfalls. One of these is that the power contained in pulsar signal is spread into multiple harmonics in calculated power spectra. The incoherent harmonic sum algorithm is one way to rectify this. The algorithm sums the power that is spread across multiple harmonics back into a single fourier bin. This increases signal-to-noise ratio of detected pulsars and allows us to detect weaker pulsars as a result. Harmonic summing also forms part of search techniques used for detecting accelerated pulsars where a two-dimensional harmonic sum is required. Or in more complex cases such as for pulsars with elliptical orbits where a three-dimensional harmonic sum might be necessary. However porting the harmonic sum to many-core architectures like GPUs is not straightforward. The main problem that must be overcome is the very unfavourable memory access pattern, which gets worse as the dimensionality of the harmonic sum increases. We present a set of algorithms for calculating the harmonic sum that are more suited to many-core architectures such as GPUs. We present an evaluation of the sensitivity of these different approaches, the performance and discuss GPU portability.
 
P Afrin Badhan, Mahmuda afrin20m@astro.umd.edu STELLAR ACTIVITY EFFECTS ON MOIST HABITABLE TERRESTRIAL ATMOSPHERES AROUND M DWARFS
    ABS: The first habitable zone (HZ) exoplanets to have their atmospheres characterized will likely be tidally-locked planets orbiting nearby M dwarfs. Future transit spectroscopy of such planets is part of the community’s plan in assessing their habitability. 3D climate modeling has shown tidally-locked HZ terrestrial planets, at the inner HZ of M dwarfs, may possess significantly enhanced water vapor content in the lower atmosphere compared to the 24 hour analog. For model M dwarfs with T  3000K in particular, such inner HZ planets have been shown to retain the moist atmosphere for low Earth-like instellation levels. This is promising for both habitability as well as our ability to spectrally detect this with the upcoming James Webb Space Telescope. However, while strong vertical mixing is expected to loft the water vapor high enough into the atmosphere, M dwarfs are typically high XUV environment. To assess whether the water vapor destruction continuously driven by such stellar UV activity levels would affect detectability, we run 1-D photochemical models of these atmospheres under varying stellar UV activity. To remain in the moist greenhouse regime as established in Kopparapu et al. 2017, we take the 3D model simulated abundances and temperature profiles for a N2-H2O dominated planet around a 3300K M dwarf. We also explore additional chemical complexity by introducing new species to our atmosphere. We find that as long as the atmosphere is well-mixed at 1 mbar and higher pressures, UV activity has no impact on our H2O detectability. We also find that even the highest UV scenario does not produce spectrally significant O3 for JWST.
 
O Albert, Kinga albert@mps.mpg.de Performance analysis of the SO/PHI software framework for on-board data reduction
    ABS: The Polarimetric and Helioseismic Imager (PHI) is the first solar spectropolarimeter that goes to deep space. It will be launched on-board the Solar Orbiter (SO) spacecraft, to orbit the Sun in highly elliptical orbits. SO/PHI has stringent requirements on its science data accuracy, while it is subjected to highly dynamic environments by the spacecraft. The orbits, however, severely limit the amount of telemetry, making the download of numerous full datasets and additional calibration data unfeasible. In addition, they also increase the command-response times of the instrument.
To overcome these limitations, SO/PHI implements autonomous on-board instrument calibration, and autonomous on-board science data reduction. The algorithms are implemented within a system of software framework and dedicated hardware implementations in reconfigurable FPGA-s. The system is designed to overcome the resource and computational power limitations of on-board computing, do secure metadata logging and warn ground support of possible errors in the data processing flow.
In this contribution we do an in-depth performance analysis of the on-board data processing system. We test multiple data pipelines on realistic data in a step by step analysis, and tune the pipeline parameters to achieve optimal results. We finally analyse the accuracy of the resulting scientific data products calculated by the instrument.
 
O ALESINA, Fabien fabien.alesina@unige.ch Exoplanets data visualization in multidimensional plots using virtual reality in DACE
    ABS: The Data Analysis Center for Exoplanets (DACE) is a web platform based at the University of Geneva (CH) dedicated to exoplanets data visualization, exchange and analysis.

This platform is based on web technologies using common programming languages like HTML and JavaScript. During the past 3 years, the plotting tools has been improved in order to display large datasets on the platform, dealing with browsers performances constraints.

The next challenge is to display the exoplanets data in multidimensional plots. The web virtual reality technology has been added on DACE, and allows the user to display the data in virtual reality devices like cardboards and headsets. 

The virtual reality is used for displaying 3D plots of synthetic planetary populations, discovered exoplanets from different archives, and 3D planetary systems with a star and its orbiting planets.

The used technologies are webVR, external GPUs called eGPUs in order to increase laptop performances, HTC vive pro headset and google cardboards.
 
O Alexov, Anastasia alexov@stsci.edu Hit the Ground Running: Data Management for JWST
    ABS: As the launch of James Webb Space Telescope (JWST) approaches a team of engineers and scientist is hard at work developing the Data Management Subsystem (DMS) for JWST with its cadre of complex imaging and spectral instruments.  DMS will perform receipt of science and engineering telemetry data; will perform reformatting, quality checking, calibration, data processing; will archive the data; will have tools for retrieving the data;  will have the capacities for reprocessing the data; will have external/public calibration tools; will provide user notification, search, and access tools for JWST science and engineering data;  will distribute data to the end user; provide extensive user analysis/visualization tools; and, will provide support for contributed data products from the community. We will give an overview of the software components, the hardware they run on, the programming languages/systems used, the complexity of the tested end to end science data flow, the current functionality of the system and what's to come for the JWST Data Management Subsystem in preparation for launch.
 
O Allen, Alice aallen@ascl.net Receiving Credit for Research Software
    ABS: Though computational methods are widely used in many disciplines, those who author these methods have not always received credit for their work. This presentation will cover recent changes in astronomy, and indeed, in many other disciplines, that include new journals, policy changes for existing journals, community resources, changes to infrastructure, and availability of new workflows that make recognizing the contributions of software authors easier. This talk will include steps coders can take to increase the probability of having their software cited correctly and steps researchers can take to improve their articles by including citations for the computational methods that enabled their research.
  ABS2 Allen, Alice Unconference session: I want to talk about...
    ABS: Unconference session: I want to talk about...
 
P Allen, Mark mark.allen@astro.unistra.fr Toward common solutions for data access, discovery and interoperability
    ABS: The ASTERICS project has supported a program of work to make enablethe data from large astronomy infrastructures (ESFRI) and pathfinder projects to become available for discovery and usage by the whole astronomical community, interoperable in the Virtual Observatory framework, and accessible with a set of common tools.  We highlight the successes and lessons learned.
 
O Ansdell, Megan ansdell@berkeley.edu Automatic Classification of TESS Planet Candidates using Deep Learning
    ABS: We present results from a NASA Frontier Development Lab (FDL) project to automatically classify exoplanet candidates identified by the Transiting Exoplanet Survey Satellite (TESS) using artificial intelligence (AI) techniques applied with compute resources provided by the Google Cloud Platform. NASA FDL is an applied AI research accelerator aimed at implementing cutting-edge AI technologies to challenges in the space sciences. NASA's TESS mission, launched in April 2018, is conducting an all-sky transit survey to discover thousands of exoplanets around bright stars that are amenable to follow-up observations needed to characterize planet sizes, masses, and even atmospheres. TESS observes sectors of the sky for 27 days, after which the TESS pipeline generates flux time series for each stellar target, identifies any transit-like signatures, and calculates diagnostic metrics for each event. Time is of the utmost importance given the short ground-based observing window following each TESS observing sector. Thus prior to the first TESS science data release, we developed a deep neural network classification system to rapidly identify planet candidates or flag false positives. We generated raw TESS data using a physics-based instrument and sky simulator, Lilith, which models the full range of instrument behavior (e.g., camera optics, readout electronics, spacecraft jigger) and includes realistic instances of stellar astrophysics (e.g., transiting planets, eclipsing binaries, stellar variability). This simulated data was then processed through the TESS pipeline to produce complete archivable data products, which we used as ground-truth data to train our deep neutral net classification system and evaluate its performance characteristics. We also show transfer learning results from a neural network trained on 150,000 Kepler light curves, then fine-tuned on simulated TESS data, and evaluate the domain gap for our model between these two distinct data sources.
 
P Araya, Mauricio mauricio.araya@usm.cl Cherenkov Shower Detection Combining Probability Distributions from Convolutional Neural Networks.
    ABS: Ground-based gamma-ray observatories such as the Cherenkov Telescope Array presents new challenges for astronomical data analysis. The dynamics of the atmosphere and the complexity of Cherenkov shower are two uncertainty sources that needed to be embraced rather than corrected.

As each telescope only has access to a separated patch, the partial information of each one has to be combined. For instance, when blots can be identified on the images, the application of Hillas parameters allows to identify the approximate direction to the projection's center. This information can be combined for several telescopes using stereoscopic reconstruction to converge on a single point. The limitation of this technique however is that it performs regressions to a predefined blot shapes, not using all the information contained in the images. Thus, deep learning techniques based on Convolutional Neural Networks have been applied with promising results. However, they rely on very large networks that process all the telescope images at once, which might not scale properly when dealing with large arrays.

We propose to run several separate instances of an smaller network for each telescope, but that are able to retrieve a probability distribution instead of approximate coordinates for the sought point. This probability distribution can be arranged by the network so it can express certainty about the direction or the distance to the center of the projection separately. The distributions retrieved by all the telescopes can be combined to get a final probability distribution. Preliminary results shows the viability of this approach to identify the center and assign a confidence value to the result.
 
X Arviset, Christophe Christophe.Arviset@esa.int ESA Science Archives and ESASky
    ABS: The ESAC Science Data Centre (ESDC), located at ESAC, Madrid, Spain, hosts all science archives from more than 20 ESA space science missions, and various more are in preparation. 

Data across the full electromagnetic spectrum can be obtained through individual astronomy archives for Gaia, XMM-Newton, Herschel, HST, Planck, ISO, EXOSAT and more recently from Lisa PathFinder as well. Furthermore, ESASky is an open science web application providing full access to the entire sky as observed by all ESA and partners astronomy missions.

The Planetary Science Archive offers a one-stop source for data from all ESA planetary missions (Rosetta, Mars Express, Exomars TGO, Venus Express, Huygens and Giotto). In addition, the heliophysics science archives provide access to data from Soho, Cluster, Ulysses, Proba-2, Double Start and ISS-SolACES.
 
O Asercion, Joseph joseph.a.asercion@nasa.gov Utilizing Conda for Fermi Data Analysis Software Releases
    ABS: The Fermi Gamma-Ray Space Telescope mission provides, via the Fermi Science Support Center (FSSC), a suite of data analysis tools to assist the high energy astrophysics community in working with Fermi data.  For many years these tools were distributed via both precompiled binaries and source tarball downloads on the FSSC’s website.  Due to the complexity of the tools and restrictions on development the downloads carried with them a large complement of third-party software which often caused package conflicts on user’s machines and bloated the size of the complete analysis package.  To alleviate these problems the Fermi development team has decided to update the distribution pipeline to utilize the Conda package management system.  This has allowed the development team to greatly reduce the software package size, eliminate a large category of bugs which once were prevalent, and target a decrease in software update turnaround/release time.  In this talk, I will outline the process the development team took to convert our legacy codebase into a Conda compatible form and outline the lessons learned throughout this process.
 
P Atemkeng Teufack, Marcellin m.atemkeng@gmail.com Baseline-dependent dimensional reduction techniques for radio interferometric big data compression
    ABS: Modern radio interferometers like MeerKAT, ASKAP, and LOFAR produce large amounts of data. The high data rates are driven by the fine time and frequency sampling of these instruments, as well as high angular resolution. For MeerKAT this amount of data is around anywhere from 64GB to 0.5 TB of row visibilities per second. The SKA will generate orders of magnitudes higher than this.
Therefore, data compression is required to save memory and improve processing time. Offringa (2016) showed that lossy compression can compress LOFAR data by a factor that is greater than 5. A natural method to compress the data is through averaging, either baseline-dependent averaging or windowing as described in Atemkeng et al. (2018). Kartik et al. (2017) showed that the data can be compressed using a Fourier dimensionality reduction model (FDRM). This is in the gridded visibilities and not in the continuous visibilities. The gridded visibilities lie on a regular grid where data for all baselines have been interpolated
together making it difficult to gauge an acceptable variance thresholding of the non-zero singular
values (see Kartik et al. (2017)). Since each baseline sees the sky differently, decorrelation is baseline dependent and the noise variance is different per-visibility: these effects cannot be taken into account in the FDRM. Some applications (e.g. transient search, or to build up a wide-FoV interferometric archive)
would require storing the raw data from all the baselines and not the gridded data.
This work will studies the different algorithms in literature for big data dimensional reduction and apply them to visibilities data. The reduction should be
baseline-dependent.
 
P Baines, Deborah dbaines@sciops.esa.int DEAVI: Dynamic Evolution Added Value Interface
    ABS: We present DEAVI, an Added Value Interface (AVI) to manage and exploit data from the ESA missions Gaia and Herschel. AVIs are software packages that provide scientists with the mechanisms to submit their own code to be executed close to the ESA mission archives. GAIA AVIs are deployed at the Gaia Added Value Interface Platform (GAVIP), a Python-based platform designed and developed by ESA and hosted at the European Space Astronomy Centre (ESAC). The proposed AVI is part of the software package being developed by Quasar Science Resources for the StarFormMapper (SFM): A Gaia and Herschel Study of the Density Distribution and Evolution of Young Massive Star Clusters project, funded by the European Union under the Horizon 2020 programme.
 
P Baumann, Matthieu matthieu.baumann@astro.unistra.fr New Python developments to access CDS services
    ABS: We will present recent developments made in the frame of the ASTERICS project and aimed at providing Python interface to CDS services and Virtual Observatory standards. Special care has been taken to integrate these developments into the existing astropy/astroquery environment.
A new astroquery.cds module allows one to retrieve image or catalogue datasets available in a given region of the sky described by a MOC (Multi Order Coverage map) object. Datasets can also be filtered through additional constraints on their metadata.
The MOCPy library has been upgraded: performance has been greatly improved, unit tests and continuous integration have been added, and the integration of the core code into the astropy.regions module is under way. We have also added an experimental support for creation and manipulation of T-MOCs which describe the temporal coverage of a data collection.
 
P Becciani, Ugo ugo.becciani@inaf.it VisIVO Visual Analytics Tool an EOSC Science Demonstrator for data discovery
    ABS: VisIVO is an integrated suite of tools and services for data discovery that include collaborative portals, mobile applications, visual analytics tool and a number of key components such as workflow applications, analysis and data mining functionalities. Space missions and ground-based facilities produce massive volumes of data and the ability to collect and store them is increasing at a higher pace than the ability to analyze them. This gap leads to new challenges in the analysis pipeline to discover information contained in the data. VisIVO Visual analytics tool for star formation regions focuses on handling these massive and heterogeneous volumes of information accessing the data previously processed by data mining algorithms and advanced analysis techniques with highly interactive visual interfaces offering scientists the opportunity for in-depth understanding of massive, noisy, and high-dimensional data. The aforementioned challenges demands an increasing archiving and computing resources as well as a federated and interoperable virtual environment enabling collaboration and re-use of data and knowledge. Thus, the connection with the European Open Science Cloud is being investigated exploiting the EGI services such as the Check-in (for federated authentication and authorization) and the Federated Cloud (for analysis and archiving services). Recently the VisIVO development has been exploited for the experimentation of cutting-edge interactive visualization technologies for the improvement of teaching and scientific dissemination. This work is being carried out as a knowledge transfer from astrophysical sciences to geological sciences in the context of an international collaboration to innovate teaching, learning and dissemination of earth sciences, using virtual reality.
 
O Berriman, Bruce gbb@ipac.caltech.edu Breathing New Life Into An Old Pipeline: Precision Radial Velocity Spectra of TESS Exoplanet Candidates
    ABS: The High Resolution Echelle Spectrograph (HIRES) at the W.M. Keck Observatory (WMKO) is one of the most effective Precision Radial Velocity (PRV) machines available to U.S. astronomers, and will play a major role in radial-velocity follow-up observations of the tens of thousands of exoplanets expected to be discovered by the Transiting Exoplanet Sky Survey (TESS) mission.  To support this community effort, the California Planet Search (CPS) team (Andrew Howard, PI) has made available a PRV reduction pipeline that will be available to all U.S. astronomers from February 2019 onwards. Operation of the pipeline has strict requirements on the manner in which observations are acquired, and these will be fully documented for users at the telescope.

The pipeline is written in IDL, and was developed over time for internal use by the CPS team in their local processing environment. Development of a modern version of this pipeline in Python is outside the scope of our resources, but it has been updated to support processing in a generic operations environment (e.g. changes to support multiple simultaneous users) We have developed a modern, Python interface to this updated pipeline, which will be accessible as a remote service hosted behind a firewall at the NASA Exoplanet Science Institute (NExScI).  Users will be able to use Python clients to access data for input to the pipeline through the Keck Observatory Archive (KOA). The pipeline which will create calibrated and extracted 1D spectra and publication-ready time series, which can be visualized and analyzed on the client side using tools already available in Python.   The Python client functions interface with the pipeline through a series of server-side web services. Users will have access to a workspace that will store reduced data and will remain active for the lifetime of the project. This design supports both reduction of data from a single night or long-term orbital monitoring campaigns.

The service is on-schedule for deployment in December 2018.

This project is a collaboration between NExScI, WMKO, KOA and CPS.
 
P Boch, Thomas thomas.boch@astro.unistra.fr Creating and managing very large HiPS: the PanSTARRS case
    ABS: HiPS (Hierarchical Progressive Surveys) is a proven Virtual Observatory standard which enables an efficient way to deliver easily potentially huge images collection and allows for fast visualisation, exploration and science applications. CDS has recently published the HiPS for PanSTARRS g and z-bands images, covering three quarter of the sky at a resolution of 250mas per pixel.
We will describe in our poster the challenges we faced and the lessons learnt in generating and distributing these HiPS made of 47 million FITS tiles, amounting to 10 trillion pixels and more than 20TB per band. In particular, we will detail the methods we developed to optimize the generation, the storage and the transfer of the HiPS.
In addition, a color HiPS, based on the two already available HiPS, has been made available and can be visualised from HiPS clients, like Aladin Desktop or Aladin Lite.
 
P Boisson, Catherine catherine.boisson@obspm.fr Multi-instrument and reproducible gamma-ray analysis
    ABS: 
 
O Bolton, Adam bolton@noao.edu Towards a National Center for Optical and Infrared Astronomy: Opportunities and Challenges in Science, Software, and Data
    ABS: Following guidance from the US National Science Foundation (NSF), the present-day operating programs of the US National Optical Astronomy Observatory (NOAO) and the Gemini Observatory are being restructured along with the future operations of the Large Synoptic Survey Telescope into a single organization, currently known as the NSF’s National Center for Optical and Infrared Astronomy (NCOA). NCOA will be the focal point for federal investment in open-access ground-based optical and infrared astronomy for the foreseeable future. I will give an overview of NCOA planning and implementation, with a focus on the new opportunities in science delivery, software, and data systems that this new organization will provide.
 
O Bonnarel, François francois.bonnarel@astro.unistra.fr ProvTAP: A TAP service for providing IVOA provenance metadata
    ABS: In the astronomical Virtual Observatory, provenance metadata provide
information on the processing history of the data. This is important to
assert quality and truthfulness of the data, and to be potentially able
to replay some of the processing steps.

The ProvTAP specification is a recently proposed IVOA Working draft
defining how to serve IVOA provenance metadata via TAP, the Table Acces
Protocol, which allows to query table and catalog services via the
Astronomical Data Query Language (ADQL). ProvTAP services should allow
finding out all activities, entities, or agents that fulfil certain
conditions.

Several implementations and developments will be presented. The CDS
ProvTAP service describes provenance metadata for HiPS generation. The
CTA ProvTAP service will provide access to metadata describing the
processing of CTA event lists. GAVO prototyped specialised query
functions that could facilitate accomplishing the goals of ProvTAP users.
 
X Borne, Kirk borne_kirk@bah.com Massive Data Exploration in Astronomy: What Does Cognitive Have To Do With It?
    ABS: There has been a tendency for astronomers to avoid unsupervised data exploration, due to the characterization of this approach as a non-scientific fishing expedition. But, a cognitive approach to massive data exploration has the potential to amplify hypothesis formulation and question generation for greater astronomical discovery. The incorporation of contextual data from other wavelengths and other surveys provides the basis for seeing interestingness in the multi-dimensional properties of sources that might otherwise appear uninteresting in a single survey database. Some suggested methods for cognitive exploration will be presented, including computer vision algorithms that are used in robotics to see patterns in the the world, but these can be used to see emergent patterns in the multi-dimensional parameter space of astronomical data.
 
X Bosch, James jbosch@astro.princeton.edu An Overview of the LSST Image Processing Pipelines
    ABS: In this talk, I'll walk through LSST's Data Release and Alert Production pipelines, highlighting how we produce various important datasets.  I'll also call attention to the algorithms where LSST will need to push the state of the art or operate qualitatively differently from previous surveys.
 
O Boussejra, Malik Olivier malik@boussejra.com aflak: Pluggable Visual Programming Environment with Quick Feedback Loop Tuned for Multi-Spectral Astrophysical Observations
    ABS: In the age of big data and data science, some may think that artificial
intelligence would bring analytical solution to every problem. However, we argue that there is still ample room left for human insight
and exploration thanks to visualization technologies. New discoveries are not made by AI (yet!). This is true in all scientific domains,
including astrophysics. With the improvements of telescopes and
proliferation of sky surveys there is always more data to analyze,
but not so many astronomers. We present aflak, a visualization
environment to open astronomical datasets and analyze them. This
paper’s contribution lies in that we leverage visual programming
techniques to conduct fine-grained, astronomical transformations,
filtering and visual analyses on multi-spectral datasets with the possibility for the astronomers to interactively fine-tune all the interacting
parameters. By visualizing the computed results in real time as the
visual program is designed, aflak puts the astronomer in the loop,
while managing data provenance at the same time.
 
O Brasseur, Clara cbrasseur@stsci.edu AstroCut: A cutout service for TESS full-frame image sets
    ABS: The Transiting Exoplanet Survey Satellite (TESS) launched this past March and will have its first data release near the end of this year. Like that of the Kepler mission, the TESS data pipeline will return a variety of data products, from light curves and target pixel files (TPFs) to large full frame images (FFIs). Unlike Kepler, which took FFIs relatively infrequently, TESS will be taking FFIs every half hour, making them a large and incredibly valuable scientific dataset. As part of the Mikulski Archive for Space Telescope's (MAST) mission to provide high quality access to astronomical datasets, MAST is building an image cutout service for TESS FFI images.  Users can request image cutouts in the form of TESS pipeline compatible TPFs without needing to download the entire set of images (750 GB). For users who wish to have more direct control or who want to cutout every single star in the sky, the cutout software (python package) is publicly available and installable for local use.

In this talk we will present the use and design of this software, in particular how we were able to optimize the cutout step. The main barrier in writing performant TESS FFI cutout software is the number of files that must be opened and read from. To streamline the cutout process we performed a certain amount of one-time work up front, which allows individual cutouts to proceed much more efficiently. The one-time data manipulation work takes an entire sector of FFIs and builds one large (~45 GB) cube file for each camera chip, so that the cutout software need not access several thousand FFIs individually.  Additionally we transpose the image cube, putting time on the short axis, thus minimizing the number of seeks per cutout. By creating these data cubes up front we achieved a significant increase in performance. We will show examples of this tool using the currently available simulated TESS data, and discuss use cases for the first data release. We will finish by discussing future directions for this software, such as generalizing it beyond the TESS mission.
 
P Brown, Matthew mbrown@keck.hawaii.edu Streamlining Pipeline Workflows: Using Python with an Object-Oriented Approach to Consolidate Aggregate Pipeline Processes
    ABS: The Keck Observatory Archive (KOA), a collaboration between the NASA Exoplanet Science Institute and the W. M. Keck Observatory, serves science and calibration data for all current and retired instruments from the twin Keck Telescopes.  In addition to the raw data, we publicly serve quick-look, reduced data products for four instruments (HIRES, LWS, NIRC2 and OSIRIS), so that KOA users can easily assess the quality and scientific content of the data.  In this paper we present the modernization of the Data Evaluation and Processing (DEP) Pipeline, our quality assurance tool to ensure science data is ready for archival.  Since there was no common infrastructure for data headers, the DEP pipeline had to evolve to accommodate new instruments through additional control paths each time an instrument or instrument upgrade was added. Over time, new modules to assist with the processing were added in a variety of languages including IDL, C, CSH, PHP, and Python.  The calls to multiple interpreters caused a lot of overhead. This project was an initiative to consolidate the DEP pipeline into a common language, Python, using an object-oriented approach. The object-oriented approach allows us to abstract out the differences and use common variables in place of instrument-specific values. As a result, for new instruments one only needs to modify a new subclass with the differing values in order to work with the pipeline. By consolidating everything to Python, we have seen an increase in efficiency, ease of operation, and ease of maintenance.
 
O Buchschacher, Nicolas nicolas.buchschacher@unige.ch No-SQL databases: An efficient way to store and query heterogeneous astronomical data in DACE.
    ABS: Data production is growing every day in all domains. Astronomy is particularly concerned with the recent instruments. While SQL databases have proven their performances for decades and still performs in many cases, it is sometimes difficult to store, analyse and combine data produced by different instruments which do not necessarily use the same data model. This is where No-SQL databases can help to solve our requirements: how to efficiently store heterogenous data in a common infrastructure ?
SQL database management systems can do a lot of powerful operations like filtering, relation between tables, sub-queries etc. The storage is vertically scalable by adding more rows in the tables but the schema has to be very well defined. In the opposite, No-SQL databases are not restrictive. The scalability is horizontal by adding more shards (nodes) and the different storage engines have been designed to easily modify the structure. This is why it is well suited in the big data era.
DACE (Data and Analysis Center for Exoplanets) is a web platform which facilitates data analysis and visualisation for the exoplanet research domain. We are collecting a lot of data from different instruments and we regularly need to adapt our database to accept new data sets with different models. We recently decided to do a major change in our infrastructure after using PostgreSQL to use CASSANDRA for the storage and Apache Solr as an indexer to do sophisticated queries among a huge number of parameters. This recent change accelerated our queries and we are now ready to accept new data sets from futur instruments and combine them with older data to do better science.
DACE is funded by the Swiss National Centre of Competence in Research (NCCR) PlanetS, federating the Swiss expertise in exoplanet research.
 
O Burnier, Julien julien.burnier@unige.ch Development, tests and deployment of web application in DACE
    ABS: The Data and Analysis Center for Exoplanets (DACE) is a web platform based at the University of Geneva (CH) dedicated to extrasolar planets data visualisation, exchange and analysis.
This platform is based on web technologies using common programming languages like HTML and Javascript for the front-end and a Java REST API for the back-end.
Over the last 6 months, the process to maintain, develop, test and deploy the applications has been dramatically improved to facilitate the maintenance and the integration of new features. The goal of such automation is to let more time to focus on development and reduce the duplicated work.
To achieve this result, we migrated our Java application to the Maven software project management and added unit tests. We implemented a pipeline on GitLab which consists of executing the tests and deploy the application in a dev environment at every commit. The front-end side is then tested using the Selenium web browser automation to simulate the user - website interactions and compare the new results with the old ones.
Once all the tests are validated, a manual action on the GitLab interface can be done to deploy the application on the official web site and we ensure the compatibility of the new features with the production version.
We are currently working to have a very complete set of tests on both back and front end in order to remove the manual part of production deployment and to have a fully automated integration of our applications.
 
X Cai, Hongbo chb@bao.ac.cn 
    ABS: 
 
O Cano, Juan Luis juanlu001@gmail.com Full Stack Data Science: Using Python to download, clean, analyze and visualize Gaia data
    ABS: Python has become of the most important programming languages in the software industry and in Astronomy, specially thanks to its popularity in the Data Science field. However, with the explosion of the Big Data movement, the tooling became quite fragmented and one single language turned out to be insufficient: efficient data access has to be done through databases using SQL, all the Hadoop stack is written in Java or Scala, and modern visualization is usually developed in JavaScript. Even if some current solutions, such as PySpark, enable the use of Python for Big Data, the deployment is often complex and the workflow fragile. This complexity is an obstacle for scientists without formal training in Software Engineering and also for the general public, particularly in Astronomy.

In recent times, the Python ecosystem has evolved to catch up with the Big Data world, and new tools have appeared to process, analyze and visualize bigger than RAM datasets. On the one hand, libraries like numba allow Python to scale up by compiling a subset of the language to assembly code, and on the other hand projects like Dask are bringing distributed and out-of-core capabilities to the usual “small data” tools of the Python in Data Science stack, allowing easy reuse of existing codebases and easy scaling to tens to thousands of nodes.

In this talk we will showcase the power of these modern Python packages to perform Full Stack Data Science on the Gaia Data Release 2. We will demonstrate how to easily download Gaia data, how to process big amounts of data on a modest laptop, and how to scale that to a distributed cloud computing cluster to increase performance with minimal cost and code changes. We will also feature special Python libraries for Big Data visualization that avoid common visualization pitfalls. Lastly, we will justify how these tools simplify the access to Astronomical data in general, discuss the limitations of the approach and talk about future developments.
 
P Cardiel, Nicolás cardiel@ucm.es Rectification and wavelength calibration of EMIR spectroscopic data with Python
    ABS: EMIR, the near-infrared camera and multi-object spectrograph operating in the spectral region from 0.9 to 2.5 microns, has been commissioned at the Nasmyth focus of the Gran Telescopio Canarias. One of the most outstanding capabilities of EMIR is its multi-object spectroscopic mode which, with the help of a robotic reconfigurable slit system, allows to take around 53 spectra simultaneously. This poster describes how important reduction steps, concerning image rectification and wavelength calibration, are performed with the help of PyEmir, the python code developed as part of the contribution of the Universidad Complutense de Madrid in this instrument.
 
P Ceballos, M.Teresa ceballos@ifca.unican.es Jitter and readout sampling frequency impact on the Athena/X-IFU performance
    ABS: The X-ray Observatory Athena is the mission selected by ESA to implement the science theme The Hot and Energetic Universe  for L2 (the second Large-class mission in ESA’s Cosmic Vision science programme).

One of the two X-ray detectors designed to be onboard Athena is X-IFU (X-ray Integral Field Unit), a cryogenic microcalorimeter based on Transition Edge Sensor (TES) technology that will provide spatially resolved high-resolution spectroscopy. X-IFU will be developed by an international consortium led by IRAP (PI), SRON (co-PI) and IAPS/INAF (co-PI) and involving ESA Member States, Japan and the United States.

X-ray photons absorbed by X-IFU detector generate intensity pulses that must be detected and reconstructed on-board to recover their energy, position and arrival time. The software prototype package (SIRENA) in development at IFCA/Spain contains a set of processing algorithms under study to get the best compromise between performance and availability of onboard computing resources. 

Recently, the baseline for the processing algorithms has been defined selecting the optimal filtering for energy reconstruction and the Single Threshold Crossing as the triggering mechanism. This combined selection provides the best compromise results for the mission requirements, based on the analysis of the simulated data.

However, the non-perfect sampling of the pulses rising-edges could result in significant errors of the reconstructed energies with the standard optimal filtering algorithm. If not corrected properly, these errors could induce a prohibitive broadening of the energy resolution.

We present here the analysis of the magnitude of this effect and propose a correction. In addition, we evaluate the impact of a reduced readout sampling frequency in the energy resolution, once the jitter correction has been applied.
 
X Chen, Wei chenwei@ihep.ac.cn An automatic data collection and analysis software for GRB studies and its result
    ABS: Gamma-ray bursts (GRBs), transient gamma rays from the sky, are currently the most luminous explosions in the universe. After its discovery in 1973, GRB has attracted more and more attentions due to its special properties, such as high energy, high redshift, and rich multi-band data.
In this work, I mainly work on the research of central engine of GRB with its large collections of GRB multi-band data. I try to constrain the central engine models based on the statistical analysis of large GRB sample. As the first step, I develop Capella, an automatic data collection and statistical analysis code dedicated for GRB studies. We then collect large amounts of GRB data with Capella from all available public data archive. 
One of the outputs of the software is the signature of a newborn Black Hole (BH) from the collapse of a supra-massive millisecond magnetar. We put special focus on the central engine of magnetar. If magnetars are the central engines of some GRBs, then several evolutionary results of magnetars are expected: (1) the immediate collapse into a BH; (2) the collapse of a supra-massive magnetar into a BH after it spins down; and (3) a stable magnetar. From GRB samples, we find two candidates exhibiting evidence of a magnetar central engine at early afterglow and a newborn starving BH at late time. Our results suggest that some magnetars may indeed collapse into a BH, which provides direct support for the magnetar and BH central engine models. Our results suggest that some magnetars may indeed collapse into a BH, which provides direct support for the magnetar and BH central engine models.
 
O Chilingarian, Igor igor.chilingarian@cfa.harvard.edu Binospec@MMT: a database-driven model of operations, from planning of observations to data reduction and archiving
    ABS: Binospec is a new optical multi-object spectrograph operated at the f/5 focus of the 6.5-m converted MMT telescope at Mt.Hopkins, Arizona commissioned in Nov/2017.  Here we describe a software system for the Binospec spectrograph driven by the PostgreSQL relational database that covers all stages of the instrument operations from the slit mask design to the data reduction pipeline and archiving and distribution of raw and reduced datasets.  We use an interactive web based front end to design slit masks, which submits the configurations directly into the database, which then are sent to the mask cutting facility.  When slit masks are installed into the spectrograph, the process is logged in the same database, which is also connected to the MMT telescope scheduler and Binospec instrument control software.  All exposures collected with the instrument are recorded into the database in real time, which allows us to perform automated pipeline data reduction.  Finally, the metadata of pipeline reduced products are ingested into the database and the products (along with raw data) are distributed to the PIs.  We demonstrate that using currently available technologies and very limited manpower it is possible to build a complete database-driven software system similar to those deployed by major space missions like Chandra, XMM, and HST.
 
O Comrie, Angus accomrie@gmail.com An HDF5 Schema for SKA Scale Image Cube Visualization
    ABS: In this paper, we describe work that has been performed to create an HDF5 schema to support the efficient visualization of image data cubes that will result from SKA Phase 1 and precursor observations. The schema has been developed in parallel to a prototype client-server visualization system, intended to serve as a testbed for ideas that will be implemented in replacements for the existing CyberSKA and CASA viewers.

Most astronomy image files are currently packaged using the FITS standard, however this has a number of shortcomings for very large images. The HDF5 technology suite provides a data model, file format, API, library, and tools, which are are all open and distributed without charge. This enables structured schemas to be created for different applications. We will show how these can be beneficial to packaging radio astronomy (RA) data. In particular, our interest is in supporting fast interactive visualization of data cubes that will be produced by the SKA telescope. Existing HDF5 schemas developed for RA data were unable to meet our requirements. The LOFAR HDF5 schema did not meet performance requirements, due to the approach of storing each 2D image plane in a separate group. The HDFITS schema serves as a starting point for an HDF5 schema that maintains round-trip compatibility with the FITS format, but lacks the additional structures required for pre-calculated and cached datasets. Therefore, we have created a new schema designed to suite our application, though this may be advantageous for other processing and analysis applications. 

The schema is similar to that of HDFITS, but extensions have been added to support a number of features required for efficient visualization of large data sets. We will discuss these extensions and provide details on performance improvements with commonly used access patterns. We will also describe real-world performance when used with our prototype visualization system.
 
O Coulais, Alain alain.coulais@obspm.fr GDL - GNU Data Language 1.0
    ABS: On behalf of the GDL team, we will be glad to use the occasion of ADASS 
2018 to announce the 1.0 release of GDL - GNU Data Language. GDL is the 
free and open-source drop-in replacement for IDL - Interactive Data 
Language. The 1.0 release, coming out 15 years after Marc Schellens has 
first made GDL public, marks several milestones attesting to the 
maturity of the project. Among these - as compared with versions 
presented at the previous editions of ADASS - there are updates in the 
plotting functionalities, major advancements in the Windows version of 
GDL, rewritten support for IDL save files, multi-threaded performance 
improvements and additions targeting compatibility with newer versions 
of IDL (including new data types).

GDL 1.0 is also the first major release following the move of GDL 
development from SourceForge (thank you for the last 15 years of 
hospitality!) to Github. This relocation was an occasion for a major 
effort targeting developer-workflow-oriented automation which helps us 
reduce entry barriers for new contributions, ensure increasing code 
coverage with automatic tests, facilitate maintenance and user support 
as well as ensure result reproducibility and traceability. Several of 
the lessons learn, insights gained and remaining challenges related 
with the development of the continuous integration process for the 
project will be reported.

Finally, the presentation will summarize current availability of 
packaged versions of GDL, compatibility with popular IDL-written 
astronomy-related software, potential directions for the upcoming 
releases, and the key areas where GDL could benefit from further user 
feedback and new contributions.
 
O Crawford, Steven scrawford@stsci.edu Triumphs and Challenges of the Astropy Project: Open Development of a Python Library for Astronomy
    ABS: The Astropy Project is a community composed of individuals and institutes that are interested in building an open source framework in Python for astronomy.  Using collaborative development tools and following an open development philosophy, the project provides an ecosystem of inter-operable packages.   We describe the project infrastructure designed to facilitate collaboration on the development of open source software and provide an update of the most recent releases.   The project has had a number of successes and challenges while being one of the largest collaboratively developed software package in astronomy.  We discuss the aspects of the project that contributed to its success while also highlighting some of the challenges to the project and future developments.
 
O Cupani, Guido guido.cupani@inaf.it Astrocook: your spectral analysis recipe book, now with a GUI
    ABS: Astrocook (http://github.com/DAS-OATs/astrocook) is a budding Python package for the analysis of quasar spectra. The project was first presented last year at ADASS and has since grown in scope and perspective with the development of a dedicated graphical user interface to control its multiple recipes for line detection, continuum determination and fitting of the absorption systems. As we will show, the Astrocook GUI is not only a user-friendly tool to access the packages functionalities, but it also provides a framework to solve the automation-vs-customization conundrum of spectral analysis. Through the GUI recipe editor, the users can now design their own workflows, tweak their parameters on test data, and launch them over extended datasets with no need for scripting. Such approach has been proved effective in fields where the human analysis fails (e.g. the assessment of completeness in the detection of the absorption systems) and is meant to set a standard for the next-generation spectrograph data analysis tools.
 
O DAI, CONG daicong2014@163.com A method to detect radio frequency interference based on convolutional neural networks
    ABS: Along with the rapid development of telecommunication, radio frequency interference (RFI) generated from diverse human produced sources like electronic equipment, cell phones, GPS and so on can contaminate the weak radio band data. Therefore, RFI is an important challenge for radio astronomy. RFI detection can be regarded a special task of image segmentation. As for RFI signals, they appears in the form of point, vertical or horizontal lines. However, most existing convolution neural networks (CNNs) perform classification tasks, where the output is the single classification label of an image. The U-Net enables classification of each pixel within the image, which is suitable and competitive for image segmentation. Thus, in this paper, we implement the U-Net of 14 layers with framework of Keras to detect RFI signals. The U-Net can perform the classification task of clean signal and RFI. Also, the U-Net is a kind of extended CNN with symmetric architecture, which consists of a contracting path to capture context information and extract features and an expanding path to get precise localization. It extracts the features of RFI for learning RFI distribution pattern and then calculates the probability value of RFI for each pixel. Then we set a threshold to get the results flagged by RFI. We train the parameter of the U-Net with “Tianlai” data(A radio telescope-array, the observing time is from 20:15:45 to 24:18:45 on 27th of September 2016, the frequency is from 744MHz to 756MHz and the number of baseline is 18528). The experimental results show that, compared with the traditional RFI flagging method, this approach performs better with satisfying accuracy and takes into account the relationship between different baselines, which contributes to correctly and effectively flag RFI.
 
O Dempsey, James james.dempsey@csiro.au Serving large scale survey data for ASKAP with SIA2 and SODA
    ABS: The Australian SKA Pathfinder (ASKAP) telescope is a survey radio telescope that will produce image cubes of over 1 TB each in size. A key problem is how astronomers will work with these huge files. The CSIRO ASKAP Science Data Archive (CASDA) has taken the approach of allowing astronomers and their scripts to work with parts of the files as well as entire files. Both sub cubes and spectra can be extracted in-place for arbitrary subsets of the cubes. The Server-side Operations for Data Access (SODA) and Simple Image Access v2 (SIA2) virtual observatory protocols provide the foundations of these capabilities. I will review how we implemented these protocols and our planned future work in this area.
 
O Dencheva, Nadia dencheva@stsci.edu GWCS - A General Approach to  Astronomical World Coordinates
    ABS: GWCS is a package for managing the World Coordinate System (WCS) of astronomical data.
It takes a general approach to the problem of expressing arbitrary transformations
 by supporting a data model which includes the entire transformation pipeline from input 
coordinates (detector by default) to world coordinates (standard celestial coordinates or physical quantities).
Transformations from the detector to a standard coordinate system are combined in a way which allows for easy 
manipulation of individual components. The framework handles 
discontinuous models (e.g. IFU data) and allows quantities that affect transforms to be treated as input 
coordinates (e.g. spectral order). It provides flexibility by allowing access to intermediate coordinate frames. 
The WCS object is serialized to a file using the language independent 
Advanced Scientific Data Format (ASDF).Alternatively the ASDF object can be encapsulated in a FITS extension.
The package is written in python and is based on astropy. It is easy to extend by 
adding new models and coordinate systems.
 
X Derriere, Sébastien sebastien.derriere@astro.unistra.fr 
    ABS: 
 
O Deshpande, Shubhankar shubhankardeshpande@hotmail.com GMRT Archival Utility for Data Analysis
    ABS: The GMRT Online Archive houses over 80 terabytes of interferometric observations obtained with
the GMRT since since the observatory began operating as a facility in 2002. The utility of this vast
data archive, likely the largest of any Indian telescope, can be significantly enhanced if first look
(and where possible, science ready) processed images can be made available to the user
community. We have initiated a project to pipeline process GMRT images in the 150, 240, 325 and
610 MHz bands. The SPAM pipeline developed by Huib Intema is being used for this purpose. A
prototyping run has been successfully completed and the results are encouraging. The thousands
of processed continuum images that we will produce will prove useful in studies of distant galaxy
clusters, radio AGN, as well as nearby galaxies and star forming regions. Besides the scientific
returns, a uniform data processing pipleine run on a large volume of data can be used in
interesting ways. For example, we will be able to measure various performance characteristics of
the GMRT telescope and their dependence on waveband, time of day, RFI environment,
backend, galactic latitude etc. in a systematic way. Since the SPAM pipeline also carries out
direction dependent modeling of ionospheric phase errors, we will also be able to measure
differential ionospheric phase delays over thousands of sightlines over the entire solar cycle to
better understand the properties of the earth's ionosphere. A variety of data products such as
calibrated UVFITS data, sky images, Hierachical Progressive Survey (HiPS) images, PyBDSF catalogs,
AIPS processing logs will be delivered to users via the GMRT online archive. Data products will be
compatible with standard Virtual Observatory protocols.
 
O Diaz, Rosa rdiaz@stsci.edu Adding Science Validation to the  JWST Calibration Pipeline.
    ABS: The JWST Calibration Pipeline is a set of steps separated into three main stages; looking to provide the best calibration for all JWST instruments, observing modes, and a wide range of science cases. Careful scientific validation and verification are necessary to determine consistency and quality of the data produced by the calibration pipeline.  With this goal in mind, the scientist at STScI have supported validation testing for most of the major builds.  Our experience with HST and the realization of the effort it would take to consistently and reliably test after each build, even after launch, motivated us to think about streamlining the process.  We started building unit tests to verify that the calibration pipeline produced the expected results. However, the need for a more in-depth scientific validation of the wide range of science cases that will be observed by JWST requires a different strategy; one that not only validates the accuracy of the data but that also provides with reliable metrics for all science cases.  We are working on defining a more complete set of science validation tests cases and simulated data that can be integrated within an automated building and testing framework; allowing full science verification and validation of the calibration pipeline in short time scales as well as quality assurance of the calibration products. 

Archiving this goal has been an arduous task, not only limited by the state of development of the software and the availability of accurate data for testing, but also by the diversity of ideas coming from a large group of scientist from different teams, resources, and conflicting schedules. In this talk, I will present the integration of the science validation testing framework within the build process. I will also discuss the challenges we faced to make this possible, the steps we took, and how this work will help us support the development of the JWST Calibration Pipeline after launch.
 
O Donaldson, Tom tdonaldson@stsci.edu Astropy and the Virtual Observatory
    ABS: The International Virtual Observatory Alliance (IVOA) has been defining standards for interoperable astronomical data exchange since 2002.  Many of these standards are being used successfully and extensively by archives and end user tools to enable data discovery and access.  Nevertheless a skepticism persists in parts of the community about the utility and even relevance of these standards, as well as the processes by which they were written.  By contrast, the Astropy Project, with its very different processes (and somewhat different goals), has been widely embraced by the community for the usefulness and usability of its interoperable Python packages.  In this talk I will discuss what these projects might learn from each other, and how more collaboration might benefit both projects and the community in general.
 
P Dowler, Patrick pdowler.cadc@gmail.com Archive-2.0: Metadata and Data Synchronisation between MAST, CADC, and ESAC
    ABS: will add later
 
P Ebisawa, Ken ebisawa@isas.jaxa.jp CALET Gamma-ray Burst Monitor web-analysis system
    ABS: 
 
P Eguchi, Satoshi satoshieguchi@fukuoka-u.ac.jp Prototype Implementation of a Web-Based Gravitational Wave Signal Analyzer: SNEGRAF
    ABS: A direct detection of gravitational waves is one of the most exciting frontiers for modern astronomy and astrophysics. Gravitational wave signals combined with classical electro-magnetic observations, known as multi-messenger astronomy, promise newer and deeper insights about the cosmic evolution of astrophysical objects such as neutron starts and black holes. To this end, we have been developing an original data processing pipeline for KAGRA, a Japanese gravitational wave telescope, for optimal detections of supernova events. As a part of our project, we have just released a web application named SuperNova Event Gravitational-wave-display in Fukuoka (SNEGRAF) in this autumn. SNEGRAF accepts the users' theoretical waveforms as a plain text file consisting of a time series of h+ and hx (the plus and cross mode of gravitational waves, respectively), then displays the input, a corresponding spectrogram, and power spectrum together with KAGRA sensitivity curve and the signal-to-noise ratio; we adopt Google Visualization API for the interactive visualization of the input waveforms. However, it is a time-consuming task to draw more than ~10^5 data points directly with JavaScript, although the number can be typical for a supernova hunt by assuming a typical duration of the event and sampling rate of the detectors; a combination of recursive decimations of the original in the server-side program and an appropriate selection of them depending on the time duration requested by the user in a web browser achieves an acceptable latency. In this poster, we present the current design, implementation and optimization algorithms of SNEGRAF, and its future perspectives.
 
O Fan, Dongwei fandongwei@nao.cas.cn A simple survey for cross-matching method
    ABS: In order to find a practicable method to build an online cross-matching service, we test several index and search methods. Indexing methods includes the directly matching, HEALPix based matching and zones algorithm. Sorted list and several search trees are also be inspected, e.g., Binary Search Tree, Red-Black Tree, B-Tree. From this survey, we can see that HEALPix based Binary Search on sorted array is the fastest and simple way to cross-match in memory, and the environment is easy to be restored from hardisk. If there are two unsorted catalog on disk, a red-black balance tree with HEALPix indices would be a good choice. But when the catalog is too big to cache in memory, the memory-hardisk-swapping much slows down the efficiency. The key is keeping more points in the memory and do the Binary Search. Not only the speed to cross-match, how to efficiently exporting the rest data columns in catalogs are also be considered. A cross-match web service build on these method is released.
 
P Feinstein, Carlos cfeinstein@fcaglp.unlp.edu.ar Extragalactic stellar photometry and the blending problem
    ABS: The images provided by the Advanced Camera for Surveys at the Hubble Space Telescope (ACS/HST) has the amazing spacial  resolution of 0.05/pixel. Therefore, it could resolve  individual stars in nearby galaxies and in particular young blue stars in associations and open clusters of the recent starburts. These data are useful for studies of the extragalactic young  population  using color magnitude diagrams (CMD) of the stellar groups. However, even with the excellent indicated spatial resolution, the blending of several stars in crowded fields can change the shape of the CMDs. Some of the blending could be handled in the cases they produce  particular features on the stellar PSF profile (e.g. abnormal sharpness, roundness, etc). But in some cases, the blend could be difficult to detect, this is the case were a pair or several stars are in the same line of sight (e.g. observed in the same pixel).      
In this work, we investigated the importance of the blending effect in several crowded regions, using both numerical simulations and real ACS/HST data. In particular, we evaluated the influence of this effect over the CMDs, luminosity functions (LFs) and reddening estimations obtained from the observations.
 
O Ferguson, Henry ferguson@stsci.edu Data Analysis Tools for JWST and Beyond
    ABS: Data Analysis tools are essential for transforming data into knowledge. They are distinct from pipeline tools in that they usually require interaction and tailoring to specific scientific needs, but parts of the analysis process can often be turned into a pipeline after an initial exploratory phase. Data analysis tools for JWST are being added to the open-source Python+Astropy, either as complete packages or as contributions to new packages. The tools include libraries to manipulate and transport complex geometric transformations (gWCS and ASDF), libraries for image analysis (imexam and photutils) and tools for analyzing spectroscopy (Specviz, MOSviz and Cubeviz).  This talk will provide an overview of the tools, highlighting areas that are ripe for collaboration, and a brief summary of the agile development process.
 
P Fernique, Pierre Pierre.Fernique@astro.unistra.fr Time in Aladin
    ABS: We present recent Aladin’s developments designed to handle and display the astronomical time dimension. Aladin was originally dedicated to visualise astronomical data in terms of spatial coordinates. Based on the same technology, we have incorporated a new dimension in Aladin: the time. This new Aladin prototype, based on the core of Version 10, incorporates two new components: a “time view” window and a “time coverage” capability. The “time view” window is a simple Aladin extension to its regular window originally designed to handle longitude VS latitude graphics. This new graphic method is dedicated to draw scatter plots where the primary axis is time and the secondary axis can be magnitude, flux, radial velocity, etc. The original spatial view and the new time view are fully interoperable allowing the users to select objects in either views to see them selected in all views simultaneously. The “time coverage” capability is based on the technology supporting the Multi-Ordered Coverage (MOCs), replacing the HEALPix space discretisation with a time scale instead. Thus the way the user manipulates time coverage is similar to space coverage manipulation, for instance performing fast coverage intersections or unions, generating a coverage from a list of sources, etc. These new capabilities are already available in the Aladin Beta version available on the Aladin CDS Web site.
 
O Fitzpatrick, Michael fitz@noao.edu The NOAO Data Lab: Design, Capabilities and Community Development
    ABS: We describe the NOAO Data Lab, a new science platform to efficiently
utilize catalog, image and spectral data from large surveys in the era of LSST.
Data Lab provides access (through multiple interfaces) to many current NOAO,
public survey and external datasets to efficiently combine traditional
telescope image/spectral data with external archives, share results and
workflows with collaborators, experiment with analysis toolkits and publish
science-ready results for community use.  The architecture, science use-case
approach to designing the system, its current capabilities and plans for
community-based development of analysis tools and services are presented.
Lessons learned in building and operating a science platform, challenges to
interoperability with emerging platforms, and scalability issues for Big Data
science are also discussed.
 
O Ford, Eric ebf11@psu.edu Improving Exoplanet Detection through Machine Learning in Wavelength and Time Presenter
    ABS: With improving precission and stability of high-resolution spectrographs for Doppler planet surveys, intrinsic stellar variability has become the key barrier to measuring the masses of temperate, rocky planets orbiting solar-type stars.  Our team is exploring the potential for machine learning to develop improved stellar activity indicators (e.g., Doppler-constrained principle components analysis in the wavelength domain) and to perform inference on spectroscopic time series (e.g., multi-variate Gaussian process regression in temporal domain).  I will report early results of applying this framework to synthetic spectroscopic time series from SOAP 2.0, describe recent progress and future plans.
 
P Gabriel, Carlos carlos.gabriel@sciops.esa.int The COSPAR Capacity Building Initiative: entering a new phase
    ABS: The Capacity Building Programme is considered today one of the flagships of COSPAR
(COmmittee for SPace Research) activities. It started in 2001 as a tentative project to widening
expertise in space science and promoting the use of data archives from space missions,
particularly in astronomy, in developing countries. In the last 17 years we have held 15 workshops
in astronomy, teaching advanced students and young researchers in developing countries how to
analyse data from diverse space missions like XMM-Newton, Chandra, Swift, Hubble, Fuse,
Gallex, Rosetta, Mars Express, Fermi, Suzaku, Herschel, Spitzer, NuStar, as well as related
science.
A first period of settling down the Programme went on for 8-9 years, in which the initial concepts
were refined, the usefulness of the workshops confirmed and an associated Fellowship created. A
second period, marked by a continuous expansion followed, this time with a Panel heading the
Programme, in which the different main space science disciplines were represented. This year we
have had a period of reflection, concluding in the COSPAR General Assembly in July, about what
can be done better, what is still missing in the Programme, and how we could use the impending
Panel reorganisation to renew and expand our objectives, initiating a new era of the CBP.
In this paper I will discuss the main decisions taken about: a) ways to better evaluate the impact of
the Programme; b) a larger interaction between the diverse space disciplines represented in it; and
c) an extension of the Programme with the aim of motivating younger students to move in the
direction of space sciences in developing countries.
 
P Geers, Vincent vincent.geers@stfc.ac.uk MIRISim: the JWST-MIRI simulator
    ABS: MIRISim is the simulator package for the JWST Mid-Infrared Instrument (MIRI), created by the MIRI European Consortium. MIRISim is designed to simulate photon propagation through MIRI, and delivers detector images consistent with the expected on-orbit performance. The simulated data has the same uncalibrated data format that will be made available to JWST observers, and include all metadata required for processing with the JWST calibration pipeline (under development by STScI). MIRISim is written in Python 3 and released as part of a custom Anaconda environment called MIRICLE, publicly available at www.miricle.org. We present an overview of MIRISim together with example simulations.
 
P Giardino, Giovanna Giovanna.Giardino@esa.int Preparing for JWST: a detailed simulation of a MOS deep field with NIRSpec
    ABS: JWST/NIRSpec will be the first multi-object spectrograph (MOS) to fly
in space and it will enable the simultaneous measurement of up to 200
spectra over the wavelength range 0.6-5.0 micron, allowing us to study
the rest-frame optical properties of large samples of galaxies out to
z~9, and the rest-frame UV out to z10. This powerful
instrument mode, however, requires careful planning of the
observations and good understanding of the processing steps necessary
to go from the detectors' count-rate images to background subtracted,
calibrated spectra.

To support the community in preparing NIRSpec MOS programs and getting
ready to analyze the data, we present here a set of simulations
closely mimicking the deep spectroscopic observations that will be
performed as part of the JADES survey, a joint effort of the NIRCam
and NIRSpec GTO teams.  The simulations are made possible by the
NIRSpec Instrument Performance Simulator software. This tool consists
of two main components: a Fourier Optics wave propagation module
coupled with a detailed model of the instruments optical geometry and
radiometric response and a detector module reproducing the noise
properties and response of NIRSpec's two H2RG sensors.  The targets
for the simulations were selected from the JWST Extragalactic Mock
Catalog, JAGUAR (Williams et al. 2018).

The simulation data package delivered here include more than 60
count-rate images corresponding to the exposures break-down of the low
and medium resolution part of one of the two NIRSpec deep-field
spectroscopic programs of the JADES survey. The simulated data
consists of three dither pointings, for 4 different instrument
configurations (low and medium resolution over the entire NIRSpec
wavelength range), plus the extracted, background subtracted, spectral
traces for each of the 370 targets and corresponding 2D-rectified
spectra and calibrated 1D spectra, as well as the mock astronomical
data used as the simulation input.
 
O Gilda, Sankalp s.gilda@ufl.edu Importance of Feature Selection in ML models
    ABS: Importance of Feature Selection in ML models

An ever looming threat to astronomical applications of ML, and especially DL, is the danger of overfitting data. In particular, we refer to the problem of stellar parameterization from low-mid resolution spectra. The preferred method to deal with this issue is to develop and use spectral indices - this requires careful measurements of equivalent widths of blended spectral lines. This is prone to use error, and does not often result in very accurate results wrt the output parameters. In this work, we tackle this problem using an iterative ML algorithm to sequentially prune redundant features (wavelength points) to arrive at an optimal set of features with the strongest correlation with each of the output variables (stellar parameters) - T_eff, log(g) and [Fe/H]. We find that even at high resolution with tens of thousands of pixels (wavelength values), most of them are not only redundant, but actually decrease the mean absolute errors (MAEs) of the model output wrt the true values of the parameters. Our results are particularly significant in this era of exploding astronomical observational capabilities, when we will undoubtedly be faced with the 'curse of dimensionality'. We illustrate the importance of feature selection to reduce noise, improve model predictions, and best utilize limited computational and hardware resources on various downsampled and degraded synthetic PHOENIX spectra, by convolving the raw high res (500,000) sources to low and mid res (2,000 - 15,000).
 
X Gill, Ranpal rgill@lsst.org 
    ABS: 
 
O Gonzalez-Nuñez, Juan jgonzale@sciops.esa.int Driving Gaia Science from the ESA Archive: DR2 to DR3
    ABS: Released 25th April, Gaia DR2 hosted in the ESA Gaia archive is leading a paradigm shift in the way astronomers access and process astronomical data in ESA archives.

An unprecedented active community of thousands of scientists is making use of the latest IVOA protocols and services (TAP, DataLink) in this archive, benefitting of remote execution and persistent, authenticated, server side services to speed up data exploration and analysis. The availability of a dedicated Python library for this purpose is connecting the archive data to new data processing workflows.

The infrastructure serving this data has been upgraded from DR1, now making use of replication, clustering, high performance hardware and scalable data distribution systems in new ways for ESA astronomical archives. VO orientation of the archive has been strengthened by the provision of Time Series in DR2 through use of a VO aware format and protocol.

In order to cover the overwhelming data volume of DR3, new services will be offered to the general astronomical community. Remote execution of code, with notebook services and access to data mining infrastructure as a service are the topics under development.

In this talk, it will be described how the current archive does enable to analyse Gaia data more effectively linked to how this is changing data analysis workflows. The infrastructure created for this purpose will be described, and the architecture and plans under implementation for DR3.
 
F Good, John jcg@ipac.caltech.edu Image Processing in Python With Montage
    ABS: The Montage image mosaic engine (http://montage.ipac.caltech.edu; https://github.com/Caltech-IPAC/Montage) has found wide applicability in astronomy research, integration into processing environments, and is an examplar application for the development of advanced cyber-infrastructure. It is written in C to provide performance and portability.  Linking C/C++ libraries to the Python kernel at run time as binary extensions allows them to run under Python at compiled speeds and enables users to take advantage of all the functionality in Python.  We have built Python binary extensions of the 59 ANSI-C modules that make up version 5 of the Montage toolkit. This has involved a  turning the code into a C library, with driver code fully separated to reproduce the calling sequence of the command-line tools; and then adding Python and C linkage code with the Cython library, which acts as a bridge between general C libraries and the Python interface. 

We will demonstrate how to use these Python binary extensions to perform image processing, including reprojecting and resampling images, rectifying background emission to a common level, creation of image mosaics that preserve the calibration and astrometric fidelity of the input images, creating visualizations with an adaptive stretch algorithm, processing HEALPix images, and analyzing and managing image metadata.

The material presented here will be made freely available as a set of Jupyter notebooks posted on the Montage GitHub page.

Montage is funded by the U. S. National Science Foundation (NSF)  under Grant Number ACI-1642453.
 
P Goz, David david.goz@inaf.it 
    ABS: 
 
O Gracia Abril, Gonzalo ggracia@sciops.esa.int Gaia DPAC Project Office: Coordinating the production of the largest star catalogue.
    ABS: The ESA Gaia satellite is creating the most accurate map ever of the Milky Way.  The second release of the Gaia archive was made public in April 2018. The impact of this release in the scientific community can be quantified with the number of papers submitted since its publication, in average around 2 papers per day based on the released data since it was published. 

The Data Processing and Analysis Consortium (DPAC) is in charge of processing the Gaia data. More than 400 scientists and engineers  distributed over 80 institutions in 20 countries work in DPAC. The  challenge is to process the, today's figures, more than 1 trillion CCD observations, of around 2 billion sources, in 6 processing centers with more than 10 processing pipelines. The data produced is  not only astrometric data but also high quality photometry, radial velocites and stellar parameters for many objects types. All these data are interdependent, output of some pipelines are required as input by other systems and, in many cases, the dependencies are cyclic.

The DPAC Project Office, together with the sub group leaders in DPAC, is responsible to coordinate the consortium activities, managing inter-dependencies between the different groups and pipelines and schedule and monitor the data deliveries across the processing centers. We will present DPAC management structure and activities, the lessons learnt in the preparation of DR2 and the challenges for future releases.
 
O Graves, Sarah s.graves@eaobservatory.org Harnessing the power of archival data to increase  scientific output: the JCMT experience.
    ABS: EAO, operators of the James Clerk Maxwell Telescope (JCMT) recently
reanalysed more than five years of publications using our data,
identifying which of our TAC-approved projects have gone onto be
published, and identifying which types of publication lead to large
numbers of follow-on publications. This has also allowed us to identify
which observations have not been published either by their PIs or by teams using our archive.


We have also examined the data products and interfaces from existing,
highly productive data collections (both those from our own
observatory and from other telescopes), identifying the features that we
believe have helped encourage uptake and.


We are currently engaged in using this information in several ways,
While this includes both 'match-making' between exciting orphaned observations
and investigators, we are also reevaluating our choice of observatory
produced data products and considering additional, simple interfaces
to encourage more use of these products.


This paper will discuss the observatory software, databases and workflow required
to do this work, as well as discussing the findings from our work that
are most relevant when running or planning observatory archives.
 
P Grishin, Kirill kirillg6@gmail.com Open-source web tools for spectroscopic and imaging data visualization for the VOXAstro initiative
    ABS: VOXAstro stands for Virtual Observatory tools for eXtragalactic Astrophysics. This initiative includes several projects such as Reference Catalog of Spectral Energy Distribution (RCSED; http://rcsed.sai.msu.ru/) and Las-Campanas Stellar Library. Here we present a set of flexible open-source tools for visualization of spectral and imaging data. Using web-visualisation libraries FlotJS and Dash we developed interactive viewers for displaying low- and high-resolution spectra of stars and galaxies, which allow one to view spectra having resolution up to R=80000 without putting a significant load on server and client sides, which is achieved by choosing the adaptive spectral binning window and dynamically preloading the datasets. We implemented a number of additional features like multiple spectra display, output of header info (e.g. stellar atmospheric parameters or stellar population properties of galaxies), display of emission lines decomposition parameters (fluxes, widths etc.) The spectral viewers can be easily embedded into any archive or database web-site. We also present a cutout service that extracts data on the fly from the UKIDSS near-infrared imaging survey and generates colour composite RGB stamps, which we use, e.g. in the RCSED web-site as an embedded service. The service is built using Astropy python library and uses IVOA SIAP to access images, which it then cutouts on the fly. In the coming years we plan to expand the capabilities of our spectroscopic and imaging visualization services and use them in future projects within VOXAstro.
 
X Guerra Noguero, Rocio rguerra@sciops.esa.int DevOps: the perfect ally for Science Operations for a large and distributed astronomy project.
    ABS: The Gaia Science Operations Centre (SOC) is an integral part of a large consortium responsible for Gaia data processing.
Serving terabytes of processed data on a daily basis to other Processing Centres across Europe makes unique demands on the processes, procedures, as well as the team itself.
In this talk I will show how we have embraced the DevOps principles to achieve our goals on performance, reliability and teamwork.
 
P Gupta, Pramod psgupta@uw.edu Computational Astrophysics with Go
    ABS: Go is a relatively new open-source language from Google. It is a compiled language and so it is quite fast compared to interpreted languages. Moreover, the creators of the language focused on minimizing  complexity. Hence, even though the language was not designed for scientific computing, its speed and simplicity make it quite attractive for scientific computing. 

In this paper, I discuss the suitability of Go for Computational Astrophysics based on using Go for Monte Carlo Radiative Transfer.
 The Go compiler has fast compile times and gives helpful error messages. Go has a standard code formatting tool gofmt so everyone's code looks the same. This makes it easier to read code written by others. The language has excellent documentation and a large number of built-in libraries. The run time performance is within a factor of two of an equivalent C, C++ or Fortran program. Go does memory management and run time checks such as array bounds checking which C, C++ and Fortran do not do. This increases the running time but it also increases the reliability of the code.


Due to Go's newness and since it is not targeted at scientific computing, there are a limited number of scientific computing libraries.  Hence, Go may not be a feasible choice for projects which depend on specialized libraries. However, for projects which are not dependent on such libraries, Go is an excellent language for Computational Astrophysics.
 
X Gwyn, Stephen gwyn.cadc@gmail.com MegaPipe 2.0: 10000 square degrees of CFHT MegaCam imaging
    ABS: MegaPipe, the MegaCam data processing pipeline at the CADC, has been
upgraded to version 2.0 and has processed 10000 square degrees of
the sky.  MegaPipe has been operating since 2008. It was originally
intended to increase the usage of archival MegaCam data by calibrating
and stacking the images as they became public. That focus moved to
include processing data from the CFHT Large Programs such as the NGVS,
OSSOS, VESTIGE and CFIS.  MegaPipe 2.0 represents several
improvements. The advent of GAIA means that the astrometric
calibration is considerably more accurate. The public release of
Pan-STARRS allows photometric calibration of images that even if they
were taken under non-photometric conditions, by using the PS1 stars as
in-field standards. Together this means that almost every MegaCam
image can be astrometrically/photometrically calibrated to sufficient
accuracy to allow stacking (30 mas and 0.01 magnitudes
respectively). The other change going to MegaPipe 2.0 is how the
images are stacked. Previously, MegaPipe only stacked images that were
taken on more or less the same part of the sky. This limited the
number of images that could stacked.  MegaPipe 2.0 instead stacks on
set of 10000x10000 pixel tiles, each half a degree square evenly
covering the whole sky. The result that twice much sky area can
stacked.  There are now 10000 square
degrees of imaging in the ugriz bands available for download at:
http://www.cadc-ccda.hia-iha.nrc-cnrc.gc.ca/en/megapipe/access/graph.html
 
O Hague, Peter prh44@cam.ac.uk BaSC - A Bayesian path to improved source finding in radio astronomy
    ABS: Widely used source detection and characterisation methods in radio interferometry borrow from those developed for optical astronomy. This approach does not account for the fundamental differences in the data, and rely on algorithms such as CLEAN to preprocess interferometric maps in such a way that information may be lost.

We present BaSC (Bayesian Source Characterisation) - a Python library that calculates likelihoods for sources based on visibilities rather than CLEANed maps, and applies Bayesian methods to generate evidence values for different models of the part of the sky being observed. BaSC is extensible, and can for example provide the ability to account for data from other wavelengths in its calculations.

We will discuss the limitations of existing methods in cases such as the confusion of two sources, and demonstrate the performance of BaSC on both simulated and real observations. 

BaSC is freely available at https://www.github.com/petehague/BASC
 
P Han, Jun hanjun@nao.cas.cn The conceptual design of amateur public observatory software framework
    ABS: As the great demand of public observatory, the amateur has built lots of observatories. They usually carry out astronomy observation in remote field stations after traveling great distances. It is not only wasting lots of time, but also not safe. As a result, we designed a hardware integration system last year to support remote observation, and the printed circuit board platform is in making.  Based on this platform, we propose a conceptual design of amateur public observatory software framework as the observation control system. It could not only satisfy multiple observation modes' requirement, but also is easy to operate and more intelligent. It will be a huge boost to astronomy education and citizen science.
 
P He, Helen hhe@cfa.harvard.edu Pixel mask Filtering of CXC Datamodel
    ABS: Datamodel(DM),  a  Chandra X-ray Center (CXC) data analysis package, 
has capabilities to display data in list, differ, sort, group, copy,
merge.  to manipulate data in filter, bin, contour, coordinate,  extract, etc.

The on-fly filtering is a widely used application to data file in
range or shapes region syntax, and a pixel masking is an extension of
the DM filtering on the conventional region.  
The pixel mask is a 2D binary image, marked in  0 (excluded) and 1+ (allowed ) arbitrary shapes, and complements the conventional analytic 
 shapes of circle, rectangle, polynomial, etc.
 
Analogous to the region filtering, the pixel masking is integrated to perform the functions of include,  exclude,
mask-and-shape intersect,  mask-and-mask union and consolidation. The
filtered mask array is stored in a data block, referenced by the
filtered region string in a data subspace of resulting file. 
We will highlight those features of the pixel mask filtering.
 
X Holman, Matthew mholman@cfa.harvard.edu The Minor Planet Center Data Processing System
    ABS: The Minor Planet Center (MPC) is the international clearing house for
all ground-based and space-based astrometric and radar observations of
asteroids, comets, trans-Neptunian objects, and outer satellites of the
giant planets. The MPC assigns designations, provides up-to-date
ephemerides, and coordinates follow-up observations for these objects.  
To meet the needs of the community, the MPC currently receives and
processes over two million observations per month and maintains a
catalog of orbits of more than 700K objects.

Although the MPC processes observations of all minor solar system
bodies, its focus is near-Earth objects (NEOs).  All MPC operations
are organized around this central function.  The MPC is required to
warn of NEOs approaching within 6 Earth Radii within the coming 6
months.  Thus, the main components of the MPC's data processing system
enable real-time identification of candidate NEOs, with possible
impact trajectories, within a much larger volume of asteroids and
other solar system objects.  A few such alerts are issued each year,
including that for ZLAF9B2/2018 LA.  In addition, The MPC facilitates
follow up observations and the coordination of observing assets for
efficient recovery searches for NEOs.

We anticipate that the data volumne will increase to a factor of 10 to
100 over the next decade as surveys such as LSST and NEOCam come
online, augmenting the already-large volume from programs such as
Pan-STARRS, the Catalina Sky Survey, NEOWISE, and ZTF.  Thus, we are
in the process of building and testing a new MPC data processing
system.  The goals are to maximize accuracy, data accessibility,
automation, and uptime while minimizing latency and maintaining
dependable archives of all data received.

In this talk I will highlight the challenges faced by the MPC, demonstrate
the key components of our data processing system, and describe a
number of algorithmic advances that support a much more efficient and
reliable system.

The MPC operates at the Smithsonian Astrophysical Observatory, part of
the Harvard-Smithsonian Center for Astrophysics (CfA), under the
auspices of the International Astronomical Union (IAU). The MPC is
100% funded by NASA as a functional sub-node of the Small Bodies Node
(SBN) of the NASA Planetary Data System at U. Maryland.
 
P Ibarra, Aitor aitor.ibarra@sciops.esa.int Standardization of object visibility and planning information through VO protocols
    ABS: The era of Multi-messenger astronomy has definitively started with the recent detection of Gravitational waves and the IceCube high energy cosmic neutrino identification. In addition, there is a substantial and increasing demand from the astronomical community to observe simultaneously in different wavebands, something which could be considered as a sub-domain of Multi-messenger science

To achieve Multi-messenger astronomy goals, the scientific community needs automatic services/tools to make the coordination of observations more efficient. Object visibility and planning information are two fundamental elements that a scientist needs to consult before being able to prepare an observation. Combining these elements from several observatories is at best a complex process and in practice this information is far from being available in a uniform way through the various observatories web pages. On the contrary, each observatory currently offers a different way to provide services related to visibility and planning.

We propose the standardization of these services to allow scientists (clients) to make queries via URLs and receive results in JSON (JavaScript Object Notation) format following and extending existing VO (Virtual Observatory) Protocols.
 
O Iwasaki, Hiroyoshi h.iwasaki@rikkyo.ac.jp A new implementation of deep neural network to spatio-spectral analysis in X-ray astronomy
    ABS: Recent rapid developments in Deep Learning, which can implicitly capture structures in high-dimensional data, will lead to the opening of a new chapter of astronomical data analysis. As a new implementation of Deep Learning techniques in the fields of astronomy, we here report our application of Variational Auto-Encoder (VAE) using deep neural network to spatio-spectral analysis of the data from the Chandra X-ray Observatory, in the particular case of Tycho’s supernova remnant. Previous applications of Machine Learning techniques to the analysis of SNRs have been limited to principal component analysis (Warren et al. 2005; Sato & Hughes 2017) and clustering without dimensional reduction (Burkey et al. 2013). We have implemented an unsupervised learning method combining VAE and Gaussian Mixture Model (GMM), where the reduction of dimensions of the observed data is performed by VAE and clustering in the feature space is by GMM. We have found that some characteristic features such as the iron knots in the southeastern region can be automatically recognized through this method. Our implementation exploits a new potential of Deep Learning in astronomical research.
 
P Jenness, Tim tjenness@lsst.org Abstracting the storage and retrieval of image data at the LSST
    ABS: Writing generic data processing pipelines requires that the algorithmic code does not ever have to know about data formats of files, or the locations of those files.  At LSST we have a software system known as the Butler, that abstracts these details from the software developer.  Scientists can specify the dataset they want in terms they understand, such as filter, observation id, date of observation, and instrument name, and the butler translates that to one or more files which are read and returned to them as a single Python object. Conversely, once they have finished processing the dataset they can give it back to the butler, with a label describing its new status, and the butler can write it in whatever format it has been configured to use.

The butler system is not LSST-specific and is entirely driven by external configuration to suit a specific use case. In this poster we describe the core features of the butler and the associated architecture.
 
P Johnston, Kyle kyjohnst2000@my.fit.edu Variable Star Classification Using Multi-View Metric Learning
    ABS: Comprehensive observations of variable stars can include time domain photometry in a multitude of filters, spectroscopy, estimates of color (e.g. U-B), etc. When it is considered that the time domain data can be further transformed via digital signal processing methodologies, the potential representations of the observed target star are limitless. If the goal is classification of observed variable stars, using this multitude of representations/views can become a challenge as many of the modern pattern classification algorithms in industry are limited to single input and single output. Presented here is an initial review of multi-view classification as applied to variable star classification, to address this challenge. The variable star UCR Starlight dataset and LINEAR dataset are used to generate a baseline performance estimate. The LM^3L algorithm is applied to a set of generic features, and the performance with regard to a generic feature space is evaluated. A matrix-variate implementation of the LM^3L algorithm is designed and presented specifically for this task as well; the matrix-variate implementations are novel developments.
 
F Joliet, Emmanuel ejoliet@ipac.caltech.edu Visualization in IRSA Services using Firefly
    ABS: NASA/IPAC Infrared Science Archive (IRSA) curates the science products of NASA's infrared and submillimeter missions, including many large-area and all-sky surveys. IRSA offers access to digital archives through powerful query engines (including VO-compliant interfaces) and offers unique data analysis and visualization tools. IRSA exploits a re-useable architecture to deploy cost-effective archives, including 2MASS, Spitzer, WISE, Planck, and a large number of highly-used contributed data products from a diverse set of astrophysics projects.

Firefly is IPAC's Advanced Astronomy WEB UI Framework. It was  open sourced in 2015, hosted at GitHub. Firefly is designed for building a web-based front end to access science archives  with advanced data visualization capabilities.The visualization provide user with an integrated experience with brushing and linking capabilities among images, catalogs, and plots.  Firefly has been used in many IPAC IRSA applications, in LSST Science Platform Portal, and in NED’s newly released interface.

In this focus demo, we will show case many data access interfaces and services provided by IRSA based on Firefly. It will demonstrate the reusability of Firefly in query, data display, and its visualization capabilities, including the newly released features of HiPS images display, MOC overlay, and the interactions between all those visualization components.
 
P Joncour, Isabelle isabelle.joncour@univ-grenoble-alpes.fr Multiscale spatial analysis of young stars complex using the dbscan clustering algorithm recursivel
    ABS: Clustering  and spatial substructures sudies of the distribution of  young stellar objects (YSOs) in star forming regions may be used as a key tracers of (1) their birth  sites and (2) their dynamical evolution with time. This work aims to provide a framework to identify and analyze  the multilevel topological substructures of star forming regions using recursively the dbscan density based clustering algorithm.

Sweeping the full range of local length scale from the scale of the whole region down to the wide pair regime, we apply the dbscan algorithm on young star spatial distribution, to identify the density connected components at each spatial scale with a high level of confidence from random expectations. We constrain the free parameters of the algorithm using correlation functions and nearest neighbor statistics. From that analysis, we derive (1) the density components spectrum defined as the number of detected density components as a function of the local scale and (2) the clusterTree object (dendrogram analogue) associated to the full multilevel topology of the density components.  To quantify the complexity of the star forming region spatial structure, we then further introduce the Strahler order as an indicator of the level and depth of the substructures.
  We use this framework to analyze three different density profile types of star forming region: fractal, Plummer and random distribution  showing that we indeed (1) recover the global density profile from the spatial point distribution analysis and (2) derive characteristic structural signatures for each type of region. These intrinsic characterizations of the topological properties of the star clusters are then used to study the topological structure of Taurus star forming complex.
 
O Juric, Mario mjuric@astro.washington.edu The ZTF Alert Stream: Lessons from the first six months of operating an LSST precursor
    ABS: The Zwicky Transient Facility (ZTF) is an optical time-domain survey that is currently generating about one million alerts each night for transient, variable, and moving objects. The ZTF Alert Distribution System (ZADS; Patterson et al.) packages these alerts, distributes them to the ZTF Partnership members and community brokers, and allows for filtering of the alerts to objects of interest, all in near-real time. This system builds on industry-standard real-time stream processing tools: the Apache Avro binary serialization format and the Apache Kafka distributed streaming platform. It leverages concepts and tools being developed for LSST (Python client libraries), with the source code publicly available on GitHub.

This talk will give an overview of the ZTF alert distribution system. We will examine lessons learned from ~six months of operating an LSST precursor alert stream (both from the operator and end-user perspective), discuss opportunities for standardization, and implications for the LSST.
 
P Kaleida, Catherine ckaleida@stsci.edu JWST Data Management Subsystem Operations:  Preparing to Receive, Process, and Archive JWST Data
    ABS: The James Webb Space Telescope (JWST) is a cornerstone in NASA's strategic plan, serving as the premier tool for studying the earliest stars and galaxies and for understanding the origins and future of the universe and the galaxies and solar systems within it. The Data Management Subsystem (DMS) is an integral part of the systems JWST needs to achieve these goals, as it serves as the interface between JWST and the astronomers who use it.  We outline the JWST DMS Operations and detail the systems and tools that will be used to ensure that the unprecedented JWST data products are of the highest quality possible and available in the archive as quickly as possible.  We also describe the rehearsals that are taking place, in order to ensure the operations systems, personnel, and procedures are ready well in advance of the spacecraft launch.
 
P Kaplan, Kyle kfkaplan@email.arizona.edu The algorithms behind the HPF and NEID pipeline
    ABS: Abstract: HPF and NEID are new high-resolution stabilized echelle spectrometers at the forefront of using radial velocity techniques to search for terrestrial mass exoplanets.  Nightly data taken at the telescopes with large format detectors must be automatically processed into radial velocities in less than 24 hours.  This requires a large investment in computer power and memory along with an automated pipeline that can check the quality of the data and handle issues without the need for human intervention.  I will present an overview of our pipeline and discuss the, sometimes novel, algorithms and techniques we use to turn the unprocessed 2D echellograms into optimally extracted 1D spectra.   These algorithms include the use of polygon clipping to rectify the curvature found in the beams on the detectors, the ability to fully account for aliasing in under-sampled data on the detector using flat lamp spectra, and the use of pixel count histograms to automatically match similar exposures and check the quality of the data.  I will also discuss how our pipeline is built up from many independent modules, making it robust against failure and allowing it to be easily modifiable.
 
P Karim, Ramsey rkarim@astro.umd.edu Alpha-X: An Alpha Shape-based Hierarchical Clustering Algorithm
    ABS: This project is an ongoing exploration into the utility of alpha shapes in describing hierarchical clustering. Alpha shapes, a concept introduced by Edelsbrunner and Mucke 1994 that relates to a generalization of convex hulls known as alpha hulls, describe boundaries of regions around point clusters that are associated (point to point) at distances less than some characteristic length scale $\alpha$. Algorithms for finding alpha shapes are based on the Delaunay triangulation of the point set, which leads to the notion of both “positive” space, in the inclusion of these triangles into the shapes, as well as “negative” space, in their exclusion. This well-defined negative space can indicate gaps in point clouds at a given $\alpha$ scale; hence the alpha shape approach can define both over- and under- densities of points. The concept of alpha shapes is a discrete approach and can thus be applied to sets of positions of stars to evaluate stellar clustering and associated voids.

One feature we are developing is the representation of point-cloud substructures as $\alpha$ values in tree representations which capture the hierarchical lineage of structure at different values of $\alpha$. With this approach, alpha shapes could be used in an alternate hierarchical cluster detection method that characterizes clusters and their complementary gaps over a range of length scales and naturally yields defined boundaries for these cluster and gap shapes. Similar approaches using this method have been successfully implemented outside of astronomy, in fields such as molecular biology, pattern recognition, and digital shape sampling (Varshney, Brooks, and Wright 1994; Edelsbrunner 2010). Our project will continue to develop and validate this methodology, comparing it to extant methods used within our field to verify that it offers novel and significant analysis products before moving into specific scientific applications.
 
P Kawasaki, Wataru wataru.kawasaki@nao.ac.jp Vissage: viewing polarisation data from ALMA
    ABS: Vissage (VISualisation Software for Astronomical Gigantic data cubEs) is a standalone FITS browser, primarily aiming to offer easy visualisation of huge, multi-dimensional FITS data from ALMA. We report its new features including basic capabilities of easily viewing polarisation data from ALMA.
 
P Kelley, Michael msk@astro.umd.edu ZChecker: Finding Cometary Outbursts with the Zwicky Transient Facility
    ABS: Cometary science benefits from wide-field time-domain optical surveys.  Aside from the discovery of new comets, such surveys can provide a better description of known objects.  For example, we can quantify intrinsic brightness variation with heliocentric distance and true anomaly (i.e., season); potentially estimate dust-to-gas ratio and its variation with time (when relevant filters are used); and identify cometary outbursts or other interesting phenomenon for follow-up.  We describe ZChecker, automated software for finding and visualizing known comets in Zwicky Transient Facility (ZTF) survey data.  ZChecker uses on-line ephemeris generation and individual data product meta-data (observation time, image corners, and the world coordinate system) to efficiently identify and download images of targets of interest in the ZTF archive.  Photometry of each target is measured, and the images rotated to place the comet-Sun vector at a constant position angle.  To help identify comets for follow-up investigations, ZChecker then scales each image of each target to a common heliocentric and geocentric distance, then combines the data into nightly and bi-weekly averages.  The difference between the two shows variations in brightness and morphology that potentially indicate, e.g., an outburst of activity or the motion of a precessing jet.  Example ZChecker output is presented, including outbursts of 29P/Schwassmann-Wachmann 1 and 64P/Swift-Gehrels, and a curved dust feature at comet C/2017 M4 (ATLAS).

Based on observations obtained with the Samuel Oschin Telescope 48-inch and the 60-inch Telescope at the Palomar Observatory as part of the Zwicky Transient Facility project.  Major funding has been provided by the U.S. National Science Foundation under Grant No. AST-1440341 and by the ZTF partner institutions.  This work is also supported by the NASA Planetary Data System Cooperative Agreement with the University of Maryland.
 
O Kent, Brian bkent@nrao.edu 3D Data Visualization in Astrophysics
    ABS: We present unique methods for rendering astronomical data - 3D galaxy catalogs, planetary maps, data cubes, and simulations.  Using tools and languages including Blender, Python, and Google Spatial Media, a user can render their own science results, allowing for further analysis of their data phase space.  We aim to put these tools and methods in the hands of students and researchers so that they can bring their own data visualizations to life on different computing platforms.
 
O Kepley, Amanda akepley@nrao.edu Auto-multithresh: A General Purpose Automated Masking Algorithm for Clean
    ABS: Generating images from radio interferometer data requires deconvolving the point spread function of the array from the initial image. This process is commonly done via the clean algorithm, which iteratively models the observed emission. Because this algorithm has many degrees of freedom, producing an optimal science image typically requires the scientist to manually mask regions of real emission while cleaning. This process is a major hurdle for the creation of the automated imaging pipelines necessary to process the high data rates produced by current and future interferometers like ALMA, the JVLA, and the ngVLA. In this talk, we present a general purpose masking algorithm called ‘auto-multithresh’ that automatically masks emission during the cleaning process. This algorithm was initially implemented within the tclean task in CASA 5.1. The tclean implementation significant performance improvements in CASA 5.3. The ‘auto-multithresh’ algorithm is in production as part of the ALMA Cycle 5 and 6 imaging pipelines. It has also been shown to work with data from telescopes like the VLA and ATCA. We describe how this algorithm works, provide a variety of examples demonstrating a success of the algorithm, and discuss the performance of the algorithm. Finally, we close with some future directions for producing science ready data products that build on this algorithm.
 
P Kong, Xiao kongx@nao.cas.cn The construction of a new stellar classification template library for the LAMOST 1D Pipeline based on LAMOST DR5
    ABS: With the ability of capturing four thousands spectra in one exposure, the Large Sky Area Multi-Object Fiber Spectroscopic Telescope (LAMOST) is a special quasi-meridian reflecting Schmidt telescope located in Xinglong Station of national Astronomical Observatory, China.
It released the fifth spectral data this year, containing 8,171,443 star, 153,090 galaxy, 51,133 QSO and 642,178 unknown type, all of which are classified by the LAMOST 1D pipeline.
This pipeline is used for spectral analysis, aiming to determine the spectral type and redshifts of the spectra observed by LAMOST by matching with spectral templates.
Generally, the performance of the stellar classification greatly depends on the quality of templates.
In this paper, we construct a new stellar template library, which is supposed to increase the number of types and the accuracy of the classification, for LAMOST based on the data from LAMOST DR5.
All the 9 million spectra from LAMOST DR5 are participated in this construction experiment and they are gathered in 500 groups using k-means clustering method.
Those group centers corresponding to spectra less than 1000 are abandoned at first.
Then the weighted average spectrum (group center) is served as the template spectrum in each group.
Initially, 417 centers are obtained.
We visually inspect all template spectra and discard 181 centers due to low spectral quality or the similarity between different group centers.
Furthermore, the types of the remained clustering centers are assigned by the subclass of spectra from LAMOST DR5.
Meanwhile, 19 templates whose subclass are difficult to determine are also abandoned.
Afterwards, we obtain a new template library containing 197 LAMOST template spectra with 82 different MK classes.
Finally, the feasibility and accuracy of using this template for classification has been verified by comparing and analyzing the classification results of several control groups of data.
 
O Kosack, Karl karl.kosack@cea.fr Data Processing Challenges for CTA
    ABS: The Cherenkov Telescope Array (CTA) Observatory will generate hundreds of petabytes of raw and simulated data during its operation---orders of magnitude more than any current gamma-ray instrument.  We present the current status of the design and prototyping efforts for a system to handle the large volume of raw data, to produce a nearly equal volume of simulated data needed to characterize the instrumental response, and to process it all to provide much more compact science
data to users. This includes our efforts at a comprehensive architectural model, as well as the use of modern technologies for distributed data processing and storage, and for a python-driven analysis.
 
P Kumar, Praveen pkumar6743@gmail.com (2+1) dimensional cosmological models in f(R,T) gravity with Λ(R,T)
    ABS: We intend to study a new class of cosmological models in f(R, T) modiﬁed theories of gravity, hence deﬁne the cosmological constant Λ as a function of the trace of the stress energy-momentum-tensor T and the Ricci scalar R, and name such a model Λ(R,T) gravity where we have speciﬁed a certain form of Λ(R, T). Λ(R, T) is also deﬁned in the perfect ﬂuid and dust case. Some physical and geometric properties of the model are also discussed. We study behaviour of some cosmological quantities such as Hubble and deceleration parameters and also the redshift function. The model is innovative in the sense that it has been described in terms of both R and T and display a better understanding of the cosmological observations.
 
X Kuulkers, Erik erik.kuulkers@esa.int Coordinating observations among ground and space-based telescopes in the multi-messenger era
    ABS: The emergence of time-domain multi-messenger (astro)physics asks for new and more efficient ways of interchanging information, as well as collaboration. Many space- and ground-based observatories have web pages dedicated to showing information about the complete observations and planned observation schedule. The aim would be to standardise the exchange of information about observational schedules and set-ups between facilities and in addition, to standardise the automation of visibility checking for multiple facilities. To reach this, we propose to use the VO protocols (ObsTAP-like) to write services to expose these data to potential client applications and to develop cross facilities visibility servers.
 
O Kyprianou, Mark kyp@stsci.edu Lessons Learned from the behemoth JWST Data Management effort
    ABS: As with any large project, there are always lessons to be learned for future endeavors that can be applied and/or considered for similar endeavors.  The development of the Data Management Subsystem (DMS) for the James Webb Space Telescope (JWST) provides such an opportunity to take advantage of experiences and events that can be applied to other missions. The DMS software and hardware tool set is extensive; it involves more than 100 staff including developers, scientists and systems engineers.  It receives science and engineering telemetry data from the spacecraft, performs quality checking, calibration, data (re-)processing; archives the data; provides user notification, search, and access tools for JWST science and engineering data;  distributes data to the end user; provides extensive user analysis/visualization tools.  We will discuss the evolution of DMS and its interaction within the Science and Operations Center (S&OC) covering keys to our success and also those that did not work out so well.
 
P Labrie, Kathleen klabrie@gemini.edu DRAGONS - Data Reduction for Astronomy from Gemini Observatory North and South
    ABS: DRAGONS, Data Reduction for Astronomy from Gemini Observatory North and South, is Gemini’s new Python-based data reduction platform.  DRAGONS offers an automation system that allows for hands-off pipeline reduction of Gemini data, or of any other astronomical data once configured.  The platform also allows researchers to control input parameters and in some cases will offer to interactively optimize some data reduction steps, e.g. change the order of fit and visualize the new solution. 

The project makes good use of other open source projects.  The data interface, Astrodata, uses at its core astropy’s NDData and io.fits.  The input parameters configuration system uses a slightly modified version of LSST's pex.config.  The project is also working with the astropy community to define the tools needed for building spectroscopic data reduction packages.

DRAGONS is used at the observatory for nighttime quality assessment.  The same software will be used for quicklook reduction of target of opportunity and LSST follow-up observations, and as the tool the researchers can use to prepare their Gemini data for analysis.
 
O Lam, Cheuk Yin c.y.lam@ljmu.ac.uk Data-Driven Pixelisation with Voronoi Tessellation
    ABS: In modern Astrophysics, Voronoi Tessellation is a rarely used as a
pixelisation scheme. While it exists, it is almost exclusively used in
signal enhancements and simulations. In Observational Astronomy, with
Gaia, ZTF, DES etc. data becoming available, LSST and Euclid coming
online in the next decade, this branch of science is becoming more and
more data-driven. HEALPix, HTM and Q3C offer excellent ways to
pixelise the celestial sphere, the implementations completely separate
the background information from the signal. There are excellent use
cases to have them independent from each other, but there are also
cases when this becomes a burden in computation when we have to
process more pixels than necessary or require post-hoc calculations to
group pixels at different resolution levels to form larger segments.
With Voronoi Tessellation, it can generate a one-to-one mapping of
data points to Voronoi cells where anywhere inside the cell is the
closest to the “governing” data point. We illustrate the application
of Voronoi Tessellation in a set of magnitude and proper
motion-limited data how it can simplify the survey properties of the
3pi Steradian Survey from the Pan-STARRS 1, where the footprint area
is imaged by the 60 CCDs at ~10^5 pointings over 3.5 years.
 
O Landais, Gilles gilles.landais@astro.unistra.fr Quality assurance in the ingestion of data into the CDS VizieR catalogue and data services
    ABS: VizieR is a reference service provided by the CDS for astronomical catalogues and tables published in academic journals, and also for associated data. Quality assurance is a key factor that guides the operations, development and maintenance of the data ingestion procedures. 
The catalogue ingestion pipeline involves a number of validation steps, which must be implemented with high efficiency to process the ~1200 catalogues per year from the major astronomy journals. These processes involve an integrated teams of software engineers, specialised data librarians (documentalists) and astronomers, and various levels of interaction with the original authors and data providers. Procedures for the ingestion of associated data have recently been improved with semi-automatic mapping of metadata into the IVOA ObsCore standard, with an interactive tool to help authors submit their data (images, spectra, time series etc.). 
We present an overview of the quality assurance procedures in place for the operation of the VizieR pipelines, and identify the future challenges of increasing volumes and complexity of data. We highlight the lessons learned from implementing the FITS metadata mapping tools for authors and data providers. We show how the quality assurance is an essential part of making the VizieR data comply with FAIR (Findable, Accessible, Interoperable and Re-useable) principles, and the necessity of quality assurance in for the operational aspects of supporting more than 300,000 VizieR queries per day through multiple interactive and programmatic interfaces.
 
O Lemson, Gerard glemson1@jhu.edu FileDB, a pattern for querying cosmological simulations
    ABS: Cosmological simulations these days consist of 100s of billions of particles and 100s of snapshots. Summary data products derived from these data sets such as cluster catalogues are now frequently stored in large, publically accessible relational databases. But there is an increasing demand for access to the raw simulation data, which however are generally too large and less suited for a purely relational treatment.
I will present FileDB, a system for querying such simulations from within a relational database, without requiring the ingestion of the raw data into the database itself. FileDB uses a special organization of the simulation snapshots along space filling curves to achieve efficient support for the main, spatial query patterns.  We have created for this purpose a query library inside SQL Server that calculates the overlap between 3D query volumes and these curves to derive efficient retrieval patterns of the required data. This library is callable in SQL though user defined functions, and has been ported to Java for use in Oracle for example.
We have implemented FileDB on the Millennium database and other simulations accessible in the online SciServer/CasJobs query service. As example I will show SQL queries joining the raw data to the halo catalogues stored in the database to determine density profiles. 
The fact that FileDB leaves the files in their original form means they can also be accessed directly on the file system. This is particularly useful for global analysis of the files, for which databases could be a bottle neck. I will show examples how SciServer/Compute provides access to these same simulations from within Jupyter notebooks running server side in Docker containers that have the data volumes directly mounted. 
Finally, the FileDB pattern easily translates to other domains such ocean circulation models, allowing the creation of multi-petabyte database in a cheap and efficient manner.
 
O Li, Changhua lich@nao.cas.cn Design of KNN Star-QSO Classification Algorithm Based on Cloud Computing
    ABS: KNN classification algorithm is a mature and simple machine learning algorithm, which is widely used in astronomical spectral classification. However, with the increasing sample size, the traditional KNN algorithm can’t meet the requirements of performance. Based on the cloud computing environment of China Virtual Observatory, the KNN algorithm is optimized based on SPARK + HDFS architecture. Through Map/Reduce, the large samples are segmented and distributed on different SPARK computing nodes, and the obvious acceleration effect is achieved. By comparing with the python-ML general algorithm, KNN's computation time can be reduced by nearly 20 times in a large scale samples
 
P Li, Jian lijian@bao.ac.cn An Visual Telescope Control Interface Based on Browser
    ABS: This work is to develop a new visual remote telescope control system, which is running on web browser. The advantage of this new system is that it not only has the function of muti-platform, also can be remote controlled by web browser. To ensure the real-time transmission of information, the WebSocket protocol is adopted in this system to transfer information between client and server. This ensures that the latest information can be obtained in time. Due to the universality of the design, this platform can be widely used in other similar telescopes.
 
P Li, Yinbi ybli@bao.ac.cn The application of Bagging TopPush method to searching for carbon stars
    ABS: In this work, we apply the Bagging TopPush method to search for carbon stars from LAMOST DR4, and found 2651 carbon stars. We firstly constructed template spectra of carbon stars in previous literature, and clustered them into three groups. Then, we investigated the pre-processing method for each group template, which have higher recall. Finally, we set parameters of the Bagging TopPush method, and searched for carbon star spectra from LAMOST DR4.
 
X Lieu, Maggie maggie.lieu@sciops.esa.int Deep learning of astronomical features with big data.
    ABS: In Astronomy, there is a tendency to build machine learning codes for very specific object detection in images. The classification of asteroids and non-asteroids should be no different than the classification of asteroids, stars, galaxies, cosmic rays, ghosts or any other artefact found in astronomical data. In computer science, it is not uncommon for machine learning to train on hundreds of thousands of object categories, so why are we not there yet? I will talk about image classification with deep learning and how we can make use of existing tools such as the ESA science archive, ESAsky and citizen science to help realise the full potential of object detection and image classification in Astronomy.
 
P Lim, Pey Lian lim@stsci.edu stginga: Ginga Plugins for Data Analysis and Quality Assurance of HST and JWST Science Data
    ABS: stginga is an image visualization package to assist in data analysis and quality assurance of science data from Hubble Space Telescope (HST) and James Webb Space Telescope (JWST).  It is based on the Ginga toolkit for building scientific viewers. In this poster, we will describe the main plugins developed for data analysis and quality assurance tasks with stginga.  We also discuss the basic outline of writing a Ginga plugin, with pointers to documentation and examples.
 
P Lin, Ganghua lgh@bao.ac.cn The quality assurance of Chinese solar physics historical observation data archives
    ABS: The archives of Chinese solar physics history observation data mainly came from the 5 observation stations, they are based on 17 kinds of observation data, among them the longest observation data has been for more then 90 years. So it should be a needed taken seriously by each data worker that how to make so many kinds of data are processed better, so that a high quality archives can be constructed, and be provided conveniently to users for using. This paper introduces the every steps of processing of the archives, for example, from the digitalization of the original data archive, archiving, data processing, to share, in the each step of processing includes the quality assurance.
 
O Loose, Marcel loose@astron.nl Agile and DevOps from the trenches at ASTRON
    ABS: A few years ago the Software development teams at ASTRON decided to adopt the Agile/Scrum software development method. We are building instruments and software that push technological boundaries. Requirements often lack sufficient detail and are subject to constant change, whilst the first data from a new instrument or early prototype become available. The unknown unknowns largely outnumber the known unknowns. Agile/Scrum has proven to be successful in situations like these. 

We stumbled and fell, but gained a lot of experience in how Agile development techniques can be used in the scientific arena. We learned what works, and what does not work. We became more and more convinced that Agile/Scrum can be very effective in the area of Scientific Software development. In this presentation I would like to take you by the hand and revisit the journey we have made, in the hope that you will learn from the mistakes that we have made, and the lessons that we have learned.
 
X Lorente, Nuria Nuria.Lorente@aao.gov.au 
    ABS: 
 
P Louys, Mireille mireille.louys@unistra.fr The IVOA provenance Data model
    ABS: This Ivoa has proposed a standard for capturing the medata of provenance in the production and distribution of astronomical data . 
we present the specification discussed for the virtual ovservatory and links to implementations .
 
P Lundquist, Michael mlundquist@email.arizona.edu Searching for Optical Counterparts to Gravitational Wave Events with the Catalina Sky Survey
    ABS: On 17 August 2017, the era of multi-messenger, gravitational wave astronomy began with the discovery of the optical counterpart to the gravitational wave event GW170817.  In this poster, we outline our software and strategy for discovering optical counterparts to future gravitational wave events using data from the Catalina Sky Survey.
 
P Lutz, Katharina katharina.lutz@astro.unistra.fr Getting ready for the fourth Asterics DADI virtual observatory school
    ABS: We present the current status of the EURO-VO tutorials in the light of the upcoming fourth Asterics DADI virtual observatory school.
 
P Ma, Xiang max@ihep.ac.cn Insight-HXMT Timing Analysis of New Black Hole Candidate MAXI J1820+070
    ABS: We present the Insight-HXMT timing analysis of the new black hole candidate MAXI J1820+070 discovered in March 2018.We find that the source  exhibits state transitions from LHS to HIMS and eventually to SIMS.  Low frequency quasi-periodic oscillations(QPO) is found in the hard state and intermediate states and its centroid frequency increase with time while Q and RMS do not show evolution. With the large effective area of Insight-HXMT at high energy band, we are able to present the energy dependence of the QPO amplitude and centroid frequency up to 200 keV which supplies more constraints on the region of QPO.
 
P Major, Brian major.brian@gmail.com 
    ABS: 
 
O Marquez, Maria Jose mariajose.marquez@eumetsat.int Galaxy Cataloguing Expert System (GCES): Use Cases
    ABS: Title: Galaxy Cataloguing Expert System (GCES): Use Cases
Lead author: María José Márquez
The observational nature of Science, especially relevant in domains such as Astronomy and Astrophysics sets important challenges in the data analysis domain. Nowadays astronomical surveys yield challenging images with crowded populations, overlapping objects, and faint detections. Here we present a Galaxy Cataloguing Expert System (GCES) to better exploit the knowledge that can be extracted from the surveys. This expert system involves the combination of appropriate AI methodologies, i.e. Bayesian inference, Voronoi tessellation and computer vision all this integrated in an automatic and configurable processing pipeline and requiring intense and intelligent computational capabilities. The GCES system addresses a multiple problem, i.e. i) label sources in an astronomical image according to the probability that each one is blended or contaminated with surrounding sources, using Voronoi tessellation and rule-based system; ii) extract photometric measurements of extended objects using contour-based approaches; and iii) carry out a probabilistic cross-match of sources from different images and bands, using all astro-photometric information available. We demonstrate here the use of the GCES in full with the most representative use cases. When the GCES system is used with the COSMOS catalogue, apparent false detections are re-discovered as real sources. This demonstrates the benefits of using artificial intelligence methodologies in the exploitation of Big Data and the consequent knowledge discovery. The paper is structured as follows: section 1 presents an introduction of the multi-axis problem and its context; section 2 describes the expert data processing and analysis pipeline (GCSE) and its specific use for the problem indicated in section 1; finally section 3 presents relevant results obtained from the different use cases with real COSMOS data sets along with a critical assessment.
 
O Martin, Thomas thomas.martin.1@ulaval.ca Putting more intelligence into the reduction and analysis of SITELLE data.
    ABS: SITELLE is the imaging Fourier transform spectrometer of the Canada-France-Hawaii Telescope (CFHT). It delivers cubes of 4 million spectra at moderate resolution (R~3000 - 10000) on a 11'x11' FOV with a spatial resolution of 0.32. 

Each data cube contains up to 34 Go of data and a handful of different astrophysical objects which cannot all be detected easily (especially at low signal-to-noise ratio) : high-redshift galaxies, planetary nebulae, emission-line stars, compact hii regions, etc. Object detection and classification is a challenge. We have developed tools based on convolutional neural networks that greatly help reasearcher to quickly find the most hidden objects.

Data reduction have also been enhanced with deep neural networks for cosmic ray detection and stars detection.

Finally, spectral model fitting can also take advantage of the python library Tensor Flow which provides GPU optimized minimization algorithm.

We will also see more of the marvelous data obtained recently.
 
O Martinez, Beatriz beatriz.martinez@esa.int Data-driven Space Science at ESA Science Data Centre
    ABS: For many scientists nowadays, the first step in doing science is exploring the data computationally. New approaches to data-driven science are needed due to the big increase of space science mission’s data in volume, heterogeneity, velocity and complexity. This applies to ESA space science missions, whose archives are hosted at the ESA Science Data Centre (ESDC). Some examples are the Gaia archive -whose size is estimated to grow up to 1PB and 6000 billion of objects-, the Solar Orbiter archive -which is expected to handle several time series with more than 500 millions of records- and the Euclid archive, which shall be able to handle up to 10PB of data.
The ESDC aims, as a major objective, to maximize the scientific exploitation of the archived data. Challenges are not limited to manage the large volume of data, but also to allow collaboration between scientists, to provide tools for exploring and mining the data, to integrate data (the value of data explodes when it can be linked with other data), or to manage data in context (track provenance, handle uncertainty and error).
ESDC is exploring solutions for handling those challenges in different areas. Specifically: storage of big catalogues through distributed databases (ex. Greenplum, Postgres-XL,…); storage of long time series in high resolution via time series oriented databases (TimeScaleDB); fulfil data analysis requirements via Elasticsearch or Spark/Hadoop; and enabling scientific collaboration and closer access to data via JupyterLab, Python client libraries and integration with pipelines using containers. In this presentation we are going to take a tour of these approaches.
 
F Mechev, Alexandar apmechev@strw.leidenuniv.nl Building LOFAR As A Service: Processing Petabytes with just a click
    ABS: The LOFAR Radio Telescope produces PetaBytes of data each year. Processing such volumes is not possible at clusters provided by academic institutions, and thus needs to be launched and managed at a High Throughput Cluster. With increasing complexity of LOFAR workflows, building and maintaining new scientific workflows on a distributed architecture becomes prohibitively time consuming. To make pipeline development and deployment easy and data processing fast, we integrate cluster middleware and LOFAR software with a leading workflow orchestration software, Airflow. The result is a flexible  application that can launch and manage LOFAR processing. With Airflow, we can easily create a service for LOFAR users to process their data transparently.
 
P Mellado, Pablo mellado@iram.es Realtime telescope and data visualization using web technologies
    ABS: When performing onsite or remote observations in a telescope, it is very critical to have a good feedback about the current status of your ongoing observation. Nowadays the web technology have evolved to allow us to get data in realtime with the advantage of using just a web browser.
 
P Michel, Laurent laurent.michel@astro.unistra.fr ALiX: An advanced search interface for AladinLite
    ABS: ALiX is a flexible catalog portal based on Aladin Lite. It it designed to use an interactive sky view as a primary selection tool. The ALiX view is constantly updated with data queried from the host database. It offers advanced functionalities allowing to mix local data with VO data. Users can draw areas of interest by hand and bookmark views.  ALiX has no dependency with any specific data source; it can be integrated in any existing portal.
 
O Micol, Alberto amicol@eso.org The new science portal and the programmatic and VO interfaces of the ESO science archive
    ABS: In 2018 new powerful ESO science archive interfaces have become available to the astronomical community. The main functionalities were already presented at the ADASS 2017, but one year has gone by not without challanges, and with some useful additions. The new archive interfaces include two main components.

The ESO Archive Science Portal: Interactive access via web pages to browse and explore the archive with interactive, iterative queries. The results are presented in real time in various tabular and/or graphic forms, including interactive previews, allowing an evaluation of the usefulness of the data which can then be selected for retrieval.

The direct database and Virtual Observatory access: The inherent limitation in the intuitive way that the web interface enables archive content to be discovered is that it is unsuited to more complex queries, such as those that include sequences with logical statements like “and”, “or” and “not”, or queries that join different sources of information. This restriction can be overcome by providing direct access to the ESO database tables. Extensive documentation is provided in terms of practical examples, which are intended to provide templates for users to customise and adapt to their specific needs.

In this first release, processed data from the LPO are supported. Future plans include expanding the support to ALMA processed data and raw data from the LPO. It is planned that these new access points will gradually replace the current ones for La Silla Paranal data, while ALMA will keep maintaining a dedicated, separate access.
 
P Million, Chase chase@millionconcepts.com tbd
    ABS: tbd
 
P Mink, Jessica jmink@cfa.harvard.edu Finding Your Place in the Cosmos with WCSTools
    ABS: The WCSTools package has been updated to include access to arbitrary-length keywords in FITS headers and the UCAC5 and GAIA catalogs. Examples of the use of those and older little -known features will be presented.
  ABS2 Mink, Jessica Data Formats BoF
    ABS: Data Formats BoF
 
P Molinaro, Marco marco.molinaro@inaf.it # Starting up a Data Model for Exoplanetary Data
    ABS: The search for, study and characterisation of extrasolar planets and planetary systems is a growing and improving field of astrophysical research. Alongside the growing knowledge on the field the data resources are also growing, both from observations and numerical simulations.
To tackle interoperability of these data an effort is starting (under the EU H2020 ASTERICS project) to delineate a data model to allow a common sharing of the datasets and collections of exoplanetary data. The data model will pick up  model components from the IVOA specifications, either existing or under investigation, and attach new ones where needed.
This contribution presents the first results in drafting the exoplanetary systems dedicated data model. It shows relationships with existing and proposed IVOA models and presents the new key components not yet available in the interoperable scenario.
The results here reported are those provided by a dedicated face-to-face meeting attended by the authors. They cover a first set of requirements and considerations and take into account aspects like the observations of exoplanetary systems, the usage of existing exoplanets catalogues, the investigation of atmospheres of confirmed exoplanets and the simulation of exoplanet's atmospheres devoted to characterize exoplanets habitability.
Use cases and requirements are also presented to form the basis of this modelling effort.
 
O Momcheva, Ivelina imomcheva@stsci.edu Hubble in the Cloud: A Prototype of a Science Platform at STScI
    ABS: The availability of high-quality, highly-usable data analysis tools is of critical importance to all astronomers as is easy access to data from our archives. In this talk I will describe the approach to developing the prototype of a new cloud-based data management environment for astronomical data reduction and analysis at STScI. I will examine the decisions we made, I will demonstrate the prototype and I will discuss what new areas of scientific exploration and discovery are opened by this platform.
  ABS2 Momcheva, Ivelina 
    ABS: 
 
O Morii, Mikio morii@ism.ac.jp Image reconstruction method for an X-ray telescope with an angular resolution booster
    ABS: We propose an image reconstruction method for an X-ray telescope with
an angular resolution booster proposed by Y. Maeda et al. at
ISAS/JAXA. The booster consists of double coded masks in front of an
X-ray mirror. In order to have a better sky image from an off-focus
image, a proper image reconstruction process must be applied. The new
image reconstruction method is based on the Bayesian statistics, where
the traditional Richardson-Lucy algorithm is extended with a prior of
sparseness and smoothness. Such a prior is desirable for astronomical
imaging because astronomical objects have variety in shape from point
sources, diffuse sources (supernova remnants, clusters of galaxies,
and pulsar wind nebula) to mixtures of them (point sources in Galactic
planes). As a result, the image resolution would be improved from a
few arcmin to 10 arcsec. The performance of the X-ray telescope is
demonstrated with simulated data: point sources and diffused X-ray
sources such as Cas A and Crab Nebula. Through the demonstration, the
angular resolution booster with the image reconstruction method is
shown to be feasible.
 
O Muench, August august.muench@aas.org [BOF] Data Citation: from Archives to Science Platforms
    ABS: As data editors of the AAS Journals we a) see steady growth in  authors using both high-level science and/or multi-purpose generic repositories for storing archival, publication related data & other materials, and b) anticipate an increase in policies that encourage/require authors to make such data persistently available at publication, and c) notice many hiccups in the workflows for data archiving with extant tools and d) are interested in how science platforms and archives plan to support the long term preservation and citation of data produced by their users.

We propose a Birds of a Feather session to discuss these issues across the spectrum of services provided by data archives through the new science platforms. The specific questions this BoF might discuss include: how do the various science platforms and data repositories plan to create persistent, citable objects for data analyzed; how can data citation via a multi-purpose science platform expose and support the concept of transitive credit (Katz 2014) to ensure that the software used on a platform is properly attributed and/or cited; what are the use cases for data citation -- what things get save by whom and when -- regardless if that data were produced on a platform or housed in monolithic space mission database.

The goal of this BoF is then to surface use cases, share implementation ideas, and move us towards policies that would better enable the citation and reuse of the data/software developed as part of the research process.

Katz (2014): https://doi.org/10.5334/jors.be
 
X Nafria, Amritpal Singh dukynafria@gmail.com How, when and where life will begin on another planet after Earth.
    ABS: It has been centuries now since humankind looked up to the sky and wondered, is there any other life out there apart from our own? Ever since that question has been asked, it's been a mystery. Our solar system has Sun and eight planets. The Sun is a Red Giant Star and in distant future it will engulf Mercury, Venus and probably Earth. The size and luminosity of the Sun is continuously increasing from its birth. By rearranging given astronomical data, this paper explores that in distant future, due to increasing size and luminosity of the Sun characteristics of the terrestrial planets will shift from one planet to another that will bring life on one of the planet in our solar system after 1 duky’s Unit. 1 duky’s unit is the time from now to the time when Mercury would get engulfed in the Sun. It unlocks so many questions like why Earth has one Moon but Mars has two, how Venus became Earth’s evil twin, what is the roll of asteroid belt in our solar system, how life began on Earth, where was it before Earth and where it will be on another planet after Earth?
 
O Nakazato, Takeshi takeshi.nakazato@nao.ac.jp New Synthesis Imaging Tool for ALMA based on the Sparse Modeling
    ABS: A new imaging tool for radio interferometry has been developed based on the sparse modeling approach. It has been implemented as a Python module operating on Common Astronomy Software Applications (CASA) so that the tool is able to process the data taken by Atacama Large Millimeter/submillimeter Array (ALMA). In order to handle large data of ALMA, the Fast Fourier Transform has been implemented with gridding process. The concept of the sparse modeling for the image reconstruction has been realized with two regularization terms: L1 norm term for the sparsity and Total Squared Variation (TSV) term for the smoothness of the resulting image. Since it is important to adjust the size of the regularization terms appropriately, the cross-validation routine, which is a standard method in statistics, has been implemented. This imaging tool runs even on a standard laptop PC and processes ALMA data within a reasonable time. The interface of the tool is comprehensible to CASA users and the usage is so simple that it consists of mainly three steps to obtain the result: an initialization, a configuration, and a processing. Remarkable feature of the tool is that it produces the solution without human intervention. Furthermore, the solution is robust in the sense that it is less affected by the processing parameters. For the verification of the imaging tool, we have tested it with two extreme examples from ALMA Science Verification Data: the protoplanetary disk, HL Tau as a typical smooth and filled image, and the lensed galaxy, SDP.81 as a sparse image. In our presentation, these results will be presented with some performance information. The comparison between our results and those of traditional CLEAN method will also be provided. Finally, our future improvement and enhancement plan to make our tool competitive with CLEAN will be shown.
 
O Navarro, Vicente vicente.navarro@esa.int ESAC Science Exploitation and Preservation Platform Reference Architecture
    ABS: At ESA, active science missions like Gaia, Planck and XMM-Newton have developed precursor systems enabling the provision of advanced applications for the execution and instantiation of data analysis pipelines. Simultaneously, developments in missions like Euclid, as well as programmes like Galileo and Copernicus are tackling the creation of cyberinfrastructures capable of acquiring, processing, distributing and analysing massive amounts of data in an effective way. These initiatives have led to the implementation of solutions, commonly known as Thematic Exploitation Platforms.

ESAC Science Exploitation and Preservation Platform (SEPP) project drives the consolidation of past experiences and future needs into a reference framework to foster research through the provision of space science data, products and services. SEPP aims at integrating information and processing assets into a single environment to deliver advanced analysis and collaboration services.

This work presents SEPP’s multi-mission reference architecture, which leverages on mainstream big data, cloud, virtualisation and container technologies to create a software as a service (SaaS) computing environment. This environment pivots around the paradigm shift characterised by the move of processing components to the data, rather than the move of data to the users.  

SEPP puts the focus on the science community, to promote their contributions and involvement in the form of data and processing extensions. It provides storage space for users to bring their data and processing code close to the archives, encouraging execution of user-customised  pipelines. Along the same lines, the integration of VO standards, tools like JupyterLab and on-demand instantiation of applications from a web based “Science App Store”, maximise interoperability and collaboration across the community.
 
X Nebot, Ada ada.nebot@astro.unistra.fr Data challenges of the VO in Time Domain Astronomy
    ABS: Surveys specifically designed to monitor the transient sky have opened the window for discovery and exploration through time domain. Source classification and transmission of the alerts for further follow-up as well as analysing possible periodicity in variable sources poses a challenge with the huge amounts of data synoptic missions are providing. We will review some of the challenges of Time Domain data and we will share some of the tools and services that are being built within the Virtual Observatory to discover, access, visualise and analyse Time Domain data, focusing in particular on Time Series.
 
P Nie, Jianyin niejy@ihep.ac.cn 
    ABS: 
 
O Nieto, Sara sara.nieto@sciops.esa.int SCIENCE EXPLOITATION IN A BIG DATA ARCHIVE: THE EUCLID SCIENTIFIC ARCHIVE SYSTEM
    ABS: Euclid is an ESA mission and a milestone to explore the dark Universe. Euclid will map the 3D distribution of up to two billion galaxies and dark matter associated with them. It will hence measure the large-scale structures of the Universe across 10 billion light years, revealing the history of its expansion and the growth of structures during the last three-quarters of its history. In total Euclid will produce up to 26 PB per year of observations. 

The Euclid Archive System is a joint development between ESA and the Euclid Consortium and is led by the Science Data Centres of the Netherlands and the ESDC (ESA Science Data Centre). The EAS is composed by three different subsystems: Data Processing System (DPS), Distributed Storage System (DSS) and Science Archive System (SAS). The SAS is being built at the ESDC and is intended to provide access to the most valuable scientific data, which is currently estimated in 10 PB of images, catalogue and spectra, after 6 years mission.

Big-data technologies, driven by nature and data volume, are transforming the way of doing scientific research towards collaborative platforms whose first goal is to enable access and process large data sets in ways that could not be done downloading the data. Distributed shared-nothing architectures for databases and data processing allowing scaling-out, joint with interactive analysis tools are currently the main technologies explored as part of the Euclid scientific archive. Some examples are: JupyterLab, Apache Spark, GreenPlum and PostgresXL. 
We will describe how Euclid in the context of the ESDC and in collaboration with the Gaia archive, envisages such a challenge to reach the scientific goals of the mission.
 
O Nikolic, Bojan b.nikolic@mrao.cam.ac.uk Acceleration of Non-Linear Minimisation with PyTorch
    ABS: Minimisation (or, equivalently, maximisation) of non-linear functions
is a widespread tool in astronomy, e.g., maximum likelihood or maximum
a-posteriori estimates of model parameters. Training of machine
learning models can also be expressed as a minimisation problem
(although with some idiosyncrasies). This similarity opens the
possibility of re-purposing machine learning software for general
minimisation problems in science.

I show that PyTorch, a software framework intended primarily for
training of neural networks, can easily be applied to general function
minimisation in science. I demonstrate this with an example inverse
problem, the Out-of-Focus Holography technique for measuring
telescope surfaces, where a improvement in time-to-solution of around
300 times is achieved with respect to a conventional NumPy
implementation. The software engineering effort needed to achieve this
speed is modest, and readability and maintainability are largely
unaffected.
 
P Obi, Ikechukwu Anthony tonykassidy_z@yahoo.com The impact of Virtual Observatory Tools in Astronomy Research  in Nigeria
    ABS: Astronomy development in Nigeria has, over the years, been hampered by lack of adequate astronomical facilities and access to data due to poor internet connectivity.
In this paper, we discuss how recent introduction of the Virtual Obervatory tools in schools and universities has brought a fast-growing interest in astronomy amongst students as well as boosting the level of astronomy research being carried out in
 arious universities.
 
P Oloketuyi, Jacob jacob.oloketuyi@ynao.ac.cn The Analysis of Periodic Variation of Sunspot Groups and the X-ray Flare Classes
    ABS: The present solar cycle 24 has been thought to be unusually quiet in many respects. Both sunspot group numbers and flare numbers show a very low level of occurrence compared to the previous cycles. This study investigated the variations and relationship exhibited between the sunspot group numbers and the solar x-ray flares classes (B, C, M and X) in the solar cycles 23 and 24. The Observations of sunspot groups and the solar x-ray flares are archived in the National Oceanic and Atmospheric Administration (NOAA) website. The study was conducted using the continuous wavelet transforms, and the cross-correlation methods. Firstly, we found that the total daily solar flare produced is asynchronous and in antiphase with the sunspot group numbers while it is positively correlated with the sunspot group numbers on cross-correlation analysis. Periodicities of short and intermediate variations were also observed with sunspot group numbers having a significant periodicity around 400 days. The phase asynchrony between the sunspot group numbers and the total flares which is a result of the drop in B and C flares during solar maximum could be attributed to overwhelming of M and X flares produced by magnetic flux system from the active region/sunspot groups.
 
P O'Toole, Simon simon.otoole@mq.edu.au data central for Astronomers
    ABS: Data Central is the AAO's flagship virtual observatory service, providing a central repository for AAT and UK Schmidt observations, survey-team derived data products and documentation. The system brings together catalogues, imaging, spectra and data cubes from dozens of surveys, providing an intuitive interface to query, explore and cross-match data sets of national and international significance. We briefly introduce the current services data central offers to help astronomers familiarize themselves with the system, including: the cutout (access imaging data from some of the world's leading facilities to quickly explore sources of interest and produce publication quality images), query (extends the existing functionality of the TAP server and now makes the catalogues hosted at data central queryable via the web interface), schema (browsing the survey-team generated metadata), documentation and cloud services. Finally we'll present the timeline for future datasets and milestones for Data Central.
  ABS2 O'Toole, Simon How do you get the most out of your teams?
    ABS: How do you get the most out of your teams?
 
P Paillassa, Maxime maxime.paillassa@u-bordeaux.fr Identifying contaminants in astronomical images using convolutional neural networks
    ABS: In this work, we propose to use convolutional neural networks to detect contaminants in astronomical images. Each contaminant is treated in a one vs all fashion. Once trained, our networks are able to detect various contaminants such as cosmic rays, hot and bad pixels defaults, saturated pixels, diffraction spikes, nebulosities, persistence effects, satellite trails, residual fringe patterns, or tracking errors and defocus in images, encompassing a broad range of ambient conditions, PSF sampling, detectors, optics and stellar density. The convolutional neural network is performing semantic segmentation: it can output a probability map, assigning to each pixel its probability to belong to the contaminant or the background class, except for tracking errors where another convolutional neural network can assign to a whole focal plane the probability that it is affected by tracking error. Training and testing data have been gathered from real data originating from various modern CCD and near-IR cameras or simulated.
 
X Parra, Jose jose.parra@alma.cl 
    ABS: 
 
X Pascual, Sergio sergiopr@fis.ucm.es Runing GTC data reduction pipelines in Jupyter
    ABS: The data reduction pipelines for the GTC (Gran Telescopio Canarias) instruments EMIR and MEGARA are based on the same framework (numina, https://zenodo.org/record/1341361). The pipeline can be run either automatically at the telescope or using a command line interface. We have added support to run the pipeline inside a Jupyter notebook, with the python interpreter. The new classes in numina provide persistent storage of reductions and querying capabilities, to retrieve older reductions
 
P Paxson, Charles cpaxson@cfa.harvard.edu Transforming Science Code into Maintainable Software, Insights into the G-CLEF Exposure Time Calculator
    ABS: We explore a common workflow in research institutions where science code is transformed into robust, maintainable, and expandable code. The case study presented is the Exposure Time Calculator (ETC) for the Giant Magellan Telescope Consortium Large Earth Finder (G-CLEF) spectrometer.  We describe the process we took to develop requirements documentation and a web application from science code.  The ETC provides a rich set of features to help the scientists estimate the performance of the instrument including: the computation of exposure time, SNR, and precision radial velocity, GUI text results, downloadable FITS standard compliant summary of results, and graphical displays.  We highlight the importance of a requirements document for information exchange between scientist and engineer, where principles and assumptions can be collaboratively understood and solidified.  As the document matures, scientists may use it to specify new requirements.  We discuss the importance of making physical interpretations of the code, of understanding and ultimately cleaning of science code magic numbers, and of comprehending the overall flow.   This detailed analysis is important since requirements morph as the project progresses.  Therefore, a modular design, especially segmenting calibration tables from source code, allow hardware engineering data upgrades without affecting software code.  Science code need not be efficient in processing speed, and illuminating black boxes allow potential speed improvements.  The software implementation is verified by comparison with hand calculations by scientists and standard data sets (in our case, flux comparisons to stellar standards of known type), and implementing best practices such as unit tests.  We analyze this process for our own purposes as well as sharing this process to assist future software engineers.
 
X Perea-Calderon, Jose Vicente jose.perea@sciops.esa.int 
    ABS: 
 
P Perez, Fernando fernando.perez@esa.int Centralisation and management of science operations procedures and test cases using SOCCI
    ABS: The Science Operations Configuration Control Infrastructure (SOCCI) is a single, highly customizable platform for system engineering, providing tools and guidelines for: requirement management, problem and change management, test management, project and document management, source version control and continuous integration. This infrastructure provides support the software development and maintenance processes of SCI-O Science Operations units at the European Space Astronomy Centre (ESAC). SOCCI reduces effort and knowledge to setup & maintain the Systems Engineering Environment (SEE) and supports the users by providing guidelines and good practices learnt from previous experiences. 

The development of SOCCI started in 2014 and it is being operationally used from June 2017. However, the range of functionalities already covered by SOCCI have been recently extended through SOCCI Evolution and SOCCI Test Framework projects. This paper describes the design, implementation and use of SOCCI for two major new functionalities: the operational procedures management and documentation including their scheduling and automatic execution; and the testing automation including the importing and exporting of test results from external test tools.
 
X Pineau, Francois-Xavier francois-xavier.pineau@astro.unistra.fr 
    ABS: 
 
X Plante, Raymond raymond.plante@nist.gov The BagIt Packaging Standard for Interoperability and Preservation
    ABS: BagIt is a simple, self-describing format for packaging related data files together which is gaining traction across many research data systems and research fields.  One of its great advantages is in how it allows a community to define and document a profile on that standard--that is, additional requirements on top of the BagIt standard that speaks to the needs of that community.  In this presentation, I will summarize the key features of the standard, highlight some important profiles that have been defined by communities, and talk about how this standard is being used as part of the NIST Public Data Repository.  I will compare and contrast the use of BagIt for enabling interoperability (e.g. for transfering data between two systems) and its use for preseravation.  I will then give an overview of the NIST BagIt profile for preservation as well as introduce a general-purpose MultiBag profile which addresses issues of evolving data and scaling to large datasets.
 
X Polsterer, Kai kai.polsterer@h-its.org 
    ABS: 
  ABS2 Polsterer, Kai ﻿A Beginners Guide to Machine Learning in Astronomy
    ABS: ﻿A Beginners Guide to Machine Learning in Astronomy
 
O Racero, Elena eracero@sciops.esa.int ESASky: A New Window for Solar System Data Exploration
    ABS: Allowing the solar system community fast and easy access to the astronomical data archives is a long-standing issue. Moreover, the everyday increasing amount of archival data coming from a variety of facilities, both from ground-based telescopes and space missions, leads to the need for single points of entry for exploration purposes. Efforts to tackle this issue are already in place, such as the ‘Solar System Object Image Search by the Canadian Astronomy Data Centre’ (CADC), plus a number of ephemeris services, such as Horizons (NASA-JPL), Miriade (IMCCE) or the Minor Planet & Comet Ephemeris Service (MPC).

Within this context, the ESAC Science Data Centre (ESDC), located at the European Space Astronomy Centre (ESAC) has developed ESASky (http://sky.esa.int), a science driven discovery portal to explore the multi-wavelength sky providing a fast and intuitive access to all ESA astronomy archive holdings. Released in May 2016, ESASky is a new web application that sits on top of ESAC hosted archives, with the goal of serving as an interface to all high-level science products generated by ESA astronomy missions. The data spans from radio to x-ray and gamma-ray regimes, with Planck, Herschel, ISO, HST, XMM-Newton and Integral missions. 

We present here the first integration of the search mechanism for solar system objects through ESASky. Based on IMCCE Eproc software for ephemeris precomputation, it allows fast discovery of photometry observations from ESA missions that potentially contain those objects within their Field Of View. In this first integration, the user is able to input a target name and retrieve on-the-fly the results for all the observations from the above-mentioned missions that match the input provided, that is, that contains within the exposure time frame the ephemerides of such objects. 

Finally, we will also discuss current developments and future plans in strong collaboration with some of the main actors in the field.
 
F Raddick, Michael jordan.raddick@gmail.com SciServer: Collabroative data-driven science
    ABS: The SciServer team is pleased to announce its final production system of SciServer, offered free to the scientific community for collaborative scientific research and education.

SciServer is an online environment for working with scientific big data, and specifically datasets hosted within our ecosystem. Researchers, educators, students, and citizen scientists can create a free account to get 10 GB of file storage space and access to a virtual machine computing environment. Users can run Python, R, or Matlab scripts in that environment through Jupyter notebooks. Scripts can be run either in Interactive mode, which displays results within the notebook, or in Batch mode, which writes results to the user’s personal database and/or filesystem.

SciServer hosts a number of datasets from various science domains; within astronomy, it features all data releases of the Sloan Digital Sky Survey (SDSS), as well as other datasets from GALEX, Gaia, and other projects. The SciServer system also incorporates the popular SkyServer, CasJobs, SciDrive, and SkyQuery astronomy research websites, meaning that SciServer Compute offers APIs to read and write from these resources. All these features ensure that computation stays close to data, resulting in faster computation therefore faster science.
SciServer also allows users to create and manage scientific collaborations around data and analysis resources. You can create groups and invite collaborators from around the world. You and your collaborators can use these groups as workspaces to share datasets, scripts, and plots, leading to more efficient collaboration.

We will present a highly interactive demo of SciServer, highlighting the latest features and with an emphasis on science use cases. Please bring your questions – let us know what you have not yet been able to do with SciServer, and we will help you do it. We are actively looking for new collaborations, feature requests, and scientific use cases. Please let us know how we can help you do your science!
 
O Ramirez, Emanuel eramirez@quasarsr.com Analysis of Astronomical Data using VR: the Gaia catalogue in 3D
    ABS: Since 2016, the ESAC Science Data Centre have been working on a number of Virtual Reality projects to visualise Gaia data in 3D. The Gaia mission is providing unprecedented astrometric measurements of more than 1 billion stars. Using these measurements, we can estimate the distance to these stars and therefore project their 3D positions in the Galaxy. A new application to analyse Gaia DR2 data will be publicly released for Virtual Reality devices during 2018. In this presentation we will give a demo of the latest version of the Oculus Rift application and will show specific use cases to analyse Gaia DR2 data as well as a demonstration on how can Virtual Reality be integrated into a data analysis workflow.
We will also show how can new input techniques such as hand-tracking can bring new levels of freedom in how we interact with data.
 
O Raugh, Anne araugh@umd.edu The PDS Approach to Science  Data Quality Assurance
    ABS: The Planetary Data System (PDS) has been mandated by NASA not merely to preserve the bytes returned by its planetary spacecraft, but to ensure those data are usable through generations - 50-100 years into the future. When PDS accepts data for archiving, it must be complete, thoroughly documented, and as far as possible autonomous within the archive (that is, everything needed to understand and use the data must be in the archive as well).  Two pillars support the PDS mission: The PDS4 Information Model, and the mandatory External Peer Review.

The PDS4 Information Model codifies metadata not just for structure, but for provenance, interpretation, and analysis as well.  The XML document structures defined for the current implementation of the model and its various constituent namespaces define minimum requirements and present best practices for describing all these aspects of the archival data. The schematic enforcement of these requirements provides a simple, automated approach to ensuring the metadata are present and well-formed.

The PDS External Peer Review is required for all data prior to acceptance for archiving.  Equivalent to the refereeing process for journal articles, The PDS External Peer Review presents the candidate data to discipline experts unaffiliated with the creation of the data.  These reviewers exercise the data in its archival form by reproducing published results, doing comparative analysis between the candidate data and similar or correlated results, and so on, using only the archival resources.  These reviewers then determine if the data are of archival quality, and where needed, formulate a list of corrections and additions required prior to archiving.

Together, the Information Model guides data preparers to producing well-formatted, well-documented data products while the External Peer Review ensures the archive submission is complete, usable, and of sufficient quality to merit permit preservation - and support - as part of the Planetary Data System archives.
 
P Renil, Rosly rosly@ska.ac.za MeerKAT: Operational Workflow and Data Analysis
    ABS: MeerKAT, the next generation radio telescope is already operational for commissioning and analysing large datasets for science community. This poster explains the operational processes in place for engineers, scientists, commissioners, etc to analyse the data-sets for further data mining that improves various processes. It is also worth to note the software processes and hardware involved, different analysis tools used for telescopic operations in producing good science data.
 
O Reustle, Alexander alexander.w.reustle@nasa.gov Automating the Fermi Software Development Process with Continuous Integration Practices
    ABS: In 2017 The Fermi LAT collaboration embarked on a plan to modernize its legacy system for software development. The goal was to employ a set of techniques known collectively as Continuous Integration to improve reliability and speed of release. This brief talk will describe the strategies and tools employed, as well as the experiences gained while migrating to this new system.
 
O Roby, Trey roby@ipac.caltech.edu New visualization Features in Firefly
    ABS: Firefly is IPAC's Advanced Astronomy WEB UI Framework. It was  open sourced in 2015, hosted at GitHub. Firefly is designed for building a web-based front end to access science archives  with data visualization capabilities. Firefly provides integrated interactive data visualization capabilities with images, catalogs, and plots.  It has been used in many IPAC IRSA applications, in LSST Science Platform Portal, and in NED’s newly released interface.

The team has been working on adding new features to Firefly visualization for last year. The most recent addition is HiPS image display, released to public in IRSA Viewer. This talk gives an overview of Firefly's new HiPS support and how we make the smooth transition between HiPS and FITS image displays.
 
O Romelli, Erik erik.romelli@inaf.it Euclidizing external tools: an example from SDC-IT on how to handle software and humanware
    ABS: Euclid is an upcoming space mission aimed at studying the dark Universe and understanding the nature of the so called Dark Matter and Dark Energy. The launch is scheduled at the moment  for the 2021. A huge amount of data, up to more than 70 PB, will be produced by the two on-board instruments (VIS and NISP) and the data processing will be a crucial aspect of the mission, especially dealing with the performance of the code involved in the scientific analysis. 

Due to the expected amount of data and estimated number of cores a distributed computing system on several Science Data Centers (SDCs) has been implemented. To ensure a uniform environment in all SDCs, any software designed for Euclid must comply with a set of common rules and must be implemented in a predefined framework. 

Not all the code is designed and implemented ex novo for Euclid purposes; usually data analysis pipelines inherit already existing software tools, designed outside the Euclid Consortium. SDCs are in charge of the integration of external code within the official Euclid software environment. 

We will present an overview of how that was done at the Italian SDC in Trieste and present some practical examples related to the integration of an external tool into the Euclid environent. The topic will be discussed taking into account the technicality and focused on the crucial, but usually ignored, aspect of human interfaces.
 
X Rots, Arnold arots@cfa.harvard.edu Data Citation
    ABS: As data citation and data preservation have gained increased prominence in recent years, we feel that it is important to continue the conversation that was started during the DOI BoF at ADASS XXVI and expand its scope. There are a number of issues regarding data citation where a coordinated approach would be advantageous to archive managers, journal publishers, and practicing researchers. These include, but are not limited to:

(1) criteria for persistent preservation and citation of data sets; 
(2) labeling of citable datasets (especially DataCite DOIs); 
(3) metadata conventions for DOIs; 
(4) repositories and platforms for persistent preservation; 
(5) discoverability of preserved data objects; 
(6) granularity of the preserved data(sets).
 
P Rubtsov, Evgenii ev.rubtcov@physics.msu.ru Stellar atmospheric parameters from full spectrum fitting of intermediate and high-resolution spectra against PHOENIX/BT-Settl synthetic stellar atmospheres
    ABS: We present a new technique implemented in IDL for determination of the parameters of stellar atmospheres using PHOENIX and BT-Settl synthetic stellar spectra. The synthetic spectra provide good coverage in the Teff, logg, [Fe/H], [α/H] parameter space over a wide wavelength range and allow to fit observed spectra of a vast majority of stars. Our procedure also determines radial velocities and stellar rotation, and it takes into account flux calibration imperfections by fitting a polynomial continuum. Thanks to using pixel fitting, we can exclude certain spectral features, which are not present in the models, such as emission lines (chromospheric emission in late-type stars or discs around Be stars). We perform a non-linear chi2 minimization with the Levenberg-Marquardt method that is applied to the entire spectrum, with the exception of areas with peculiarities: emission lines, model shortcomings (incompleteness of the spectral line lists used for the atmospheric model calculation). We take into account systematic errors of the surface gravity estimates introduced by synthetic atmospheres by applying a correction computed from the comparison of our results with those obtained using asteroseismology. We present the comparative statistical analysis of optical spectral libraries ELODIE, INDO-US, MILES, UVES-POP, and a new near-infrared Las Campanas Stellar Library and discuss prospective applications of our technique.
 
O Rusholme, Benjamin rusholme@caltech.edu The Zwicky Transient Facility Data System
    ABS: The Zwicky Transient Facility (ZTF) is a new time-domain survey that entered operations in 2018. Building on the highly-successful legacy of the Palomar Transient Factory (PTF), ZTF uses a new camera with a 47 square degree field of view and fast readout electronics to survey more than an order of magnitude faster than PTF. This required a revamped data system to rapidly process the imaging data into a high-quality stream of machine-learning vetted transient alerts for follow up. ZTF is already producing half a million alerts per night which are being distributed to public brokers using Apache Kafka following the proposed LSST model
 
P Ryan, P. Wesley wes@ascl.net Schroedinger’s code: Source code availability and transparency in astrophysics
    ABS: Astronomers use software for their research, but how many of the codes they use are available as source code? We examined a sample of 166 papers from 2015 for clearly identified software use, then searched for source code for the software packages mentioned in these research papers. We categorized the software to indicate whether source code is available for download and whether there are restrictions to accessing it, and if source code was not available, whether some other form of the software, such as a binary, was. Over 40% of the source code for the software used in our sample was not available for download.

As URLs have often been used as proxy citations for software and data, we also extracted URLs from one journal’s 2015 research articles, removed those from certain long-term reliable domains, and tested the remainder to determine what percentage of these URLs were accessible in September and October, 2017. We repeated this test a year later to determine what percentage of these links were still accessible. This poster will present what we learned about software availability and URL accessibility in astronomy.
 
O Salgado, Jesus Jesus.Salgado@sciops.esa.int Gaia DR2 and the Virtual Observatory: VO in operations new era
    ABS: During the last decade, the IVOA (International Virtual Observatory Alliance) has been tasked with the difficult task of defining standards to interchange astronomical data. These efforts have been supported by many IVOA partners in general and by the ESAC Science Data Centre (ESDC) in particular, that have been collaborating in the definition of standards and in the development of astronomical VO-inside archives. New ESDC archives, like Gaia, ESASky and the ones in development like Euclid, makes use of VO standards not only as a way to expose the data but, also, as the architectural design of the system.

The Gaia Data Release 2 archive, has been a global effort done not only concentrated into the central archive at ESAC, but also as a collaboration with partner data centers like CDS, ARI, ROE and other members of DPAC. All Gaia partners make use of VO protocols to expose Gaia data as a principle. With this release, the level of dissemination and community endorsement of the VO protocols have entered into a new phase. The percentage of the Gaia expert community that makes use of VO standards has been increased to an unprecedented level of use;  34,000 users accessing the ESA Gaia Archive interface; over 5,000 advanced users sending more than 1,5 million data analysis queries during the only the first week and just counting the ESA Gaia Archive.

These standards are offered to the community in a transparent way (like SAMP or VOSpace to interchange data), as VO procotols extensions like the Tabular Access Procol extension (TAP+) or as direct use, like the Astronomical Data Access Language (ADQL) that the users learn in order to implement data mining scientific use cases that were almost impossible in the past.

We will describe how VO protocols simplify the work of design and implementation of the astronomical archives and the current level of endorsement by the scientific community.
 
P Sanguillon, Michele Michele.Sanguillon@umontpellier.fr An overview of the OVGSO data centre
    ABS: The OVGSO (Observatoire Virtuel du Grand Sud-Ouest: https://ov-gso.irap.omp.eu/) is one of the six French Astrophysical Data Center recognized by INSU (Institut National des Sciences de l’Univers) since 2013. It aims at providing a common environment for all the Astrophysical researches, based on observed/theoretical/modeled data access and tools interoperability. The OVGSO gathers five
different themes: Sun-Earth (STORMS: https://stormsweb.irap.omp.eu/, CLIMSO-DB: http://climso.irap.omp.eu/), planetary plasmas (CDPP: http://cdpp.irap.omp.eu/), interstellar medium (CASSIS: http://cassis.irap.omp.eu/, CADE: http://cade.irap.omp.eu/, KIDA: http://kida.obs.ubordeaux1.fr/), stellar spectra (PolarBase: http://polarbase.irap.omp.eu/, Pollux: http://pollux.oreme.org/), high energy astrophysics (SCC-XMM: http://xmmssc.irap.omp.eu/). We will present in the poster the different tools that have been developed and the OV standards and protocols that have been used within this Data Center. For example, spectroscopic observations can be retrieved using SSAP (Single Spectral Access Protocol) to access the IVOA services, to reach some observed data and link them to theoretical atomic and molecular databases such as VAMDC (Virtual Atomic and Molecular Data Centre: http://www.vamdc.eu/) using TAP (Table Access Protocol).
SAMP (Simple Application Messaging Protocol) is also used to send spectra on the tools developed by OVGSO.
The goal of the OVGSO is to share common tools for different thematics in astrophysics for a better use of the astrophysical data. This need has revealed to be crucial with the wealth of high-spatial/
spectral resolution spectra for the past 15 years, with new generation of ground/space-based observatory. Dealing with a large quantity of data leads to optimized tools and a better and common access to the many databases.
 
P Santos, Rafael rafael.santos@inpe.br A hybrid neural network approach to estimate galaxy redshifts from multi-band photometric survey.
    ABS: TBD
 
P Schaaff, Andre andre.schaaff@astro.unistra.fr Chatting with the astronomical data services.
    ABS: In our everyday life, we use increasingly the voice to interact with assistants for heterogeneous requests (weather, booking, shopping, etc.). We present our experiments to apply the Natural Language Processing to the querying of astronomical data services. It is of course easy to prototype something. But is it realistic to propose it as a new way of interaction in a near future, as an alternative to the traditional forms exposing parameter fields, check boxes, etc.? 
To answer to this question, it is necessary to answer before to the most fundamental question: is it possible to satisfy professional astronomers needs through this way? 
We have not started from scratch as we have useful tools and resources (name resolver, authors in Simbad, missions and wavelengths in VizieR, etc.) and the Virtual Observatory (VO) brings us standards like TAP, UCDs, ..., implemented in the CDS services. 
The interoperability, enabled by the VO, is a mandatory backbone. We explain how it helps us to query our services in Natural Language. And how it will be possible in a further step to query the whole VO through this way. We present our pragmatic approach based on a chatbot interface (involving Machine Learning) to reduce the gap between good and imprecisely/ambiguous queries. Comments (necessarily enthusiastic) are welcome. Collaborations too.
 
O Sealey, Katrina katrina.sealey@aao.gov.au Bringing together the Australian sky - coordination and interoperability challenges of the All-Sky Virtual Observatory
    ABS: The Australian All-Sky Virtual Observatory (ASVO) consists currently of 5 nodes. There are 2 nodes with optical astronomical data; Data Central (MQ) and Skymapper (ANU). There are 2 nodes with radio data; Murchison Wide Field Array (MWA, Curtin) and CSIRO ASKAP Science Data Archive (CASDA, CSIRO). The last node is the Theoretical Astrophysical Observatory (TAO, Swin). These 5 nodes work together under the unified ASVO. 

The Australian astronomical user community is driving multi-node and multi-wavelength use cases, for example, querying Data Central spectroscopic data with Skymapper imaging data. Meeting the user requirements of the community comes with complexities and challenges. Some of the challenges we are facing include a single sign-on (unified authorisation/authentication) and the querying and representation of very different remote data, such as, overlaying GaLactic and Extragalactic All-sky MWA Survey (GLEAM) data stored in Western Australia with imaging data stored in Eastern Australian states. This presentation will discuss the challenges and successes in both co-ordinating the Australian ASVO and providing interoperability across the 5 nodes.
 
P Servillat, Mathieu mathieu.servillat@obspm.fr The IVOA Provenance Data Model
    ABS: The IVOA Provenance Data Model
 
O Shawhan, Peter pshawhan@umd.edu BoF: Data analysis challenges for multi-messenger astrophysics
    ABS: The recent multi-messenger observations of gravitational-wave and high-energy neutrino sources together with electromagnetic signatures have brought completely new ways for observing the Universe. These are promising a future where advancing physics and astronomy will be enabled by combining observations and data from all across the electromagnetic spectrum, gravitational-waves and neutrinos. We consider the challenges the field is facing in fully utilizing data for multi-messenger astrophysics. Such data come from heterogeneous detector networks and standards and their analysis is often time-critical to guide further observations. In this area, science capabilities depend on the interplay among observation, theory and computational/modeling work. Advances in data science and computing present additional opportunities and considerations in analyzing such data. We invite all ADASS participants in this Birds of a Feather session to engage in an informal discussion on the challenges and opportunities in data analysis for multi-messenger astrophysics.
 
O Shen, Robert robert.shen@astronomyaustralia.org.au ASVO MWA project: lower technical barrier to access MWA data
    ABS: The Murchison Widefield Array (MWA) is a low-frequency radio telescope operating between 80 and 300 MHz. The MWA project aims to perform large surveys of the entire southern sky hemisphere and acquire deep observations on targeted regions. For the last five years, the MWA has generated more than 20Pb of raw data, but there has been no easy way for the astronomical community to access this data or for MWA researchers to share data among their collaborators. The ASVO MWA project builds on the previous pilot project and aims to address these issues by reducing the technical barriers for the community to discover, download and use both public and proprietary MWA data. Specifically, the ASVO MWA project has four service components: (1) Calibration component, which allows users to apply a system determined calibration solution to their selected  data products using an IVOA compliant UWS based pipeline; (2) Authentication component, which provides users with a single sign-on via EduGAIN by collaborating with Spherical Cow Group (SCG) on their existing Identify Management Project; (3) VOSpace component, which provides  an IVOA compatible VOSpace service and functionality to enable users to interact with their data in an IVOA compatible environment; (4) MWA web development component, which provides a modern web user experience by enhancing  the existing ASVO MWA pilot GUI and system management interfaces. 
The ASVO MWA project will lower the barrier for astronomers to enter the world of big data analysis, processing and computational modelling. Further steps in the development of the ASVO MWA over the next few years will enhance ASVO MWA functionality and act as an early pilot towards the forthcoming SKA regional centre. In this talk, we will present details of the implementation and look-and-feel of the ASVO MWA project.
 
P Shin, Min-Su msshin@kasi.re.kr Applications of the in-memory database Redis in processing transient event alerts
    ABS: We present results of using the in-memory database Redis in processing transient event alerts. The Redis works in two different ways for processing alerts. First, the publication-subscription model in the Redis allows us to adopt it as a message delivery system for multiple local alert clients. Second, we use the features of indexing and storing geolocation information in the Redis to enable low-latency matching of transient locations with custom catalogs. The current system collects event alerts by using VOEvent streams and detecting changes in web pages/feeds. We also introduce our efforts of migrating the system from the Redis message delivery environment to the NATS-based message processing configuration as well as application of Uber's H3 spatial indexing model instead of the Redis geolocation support.
 
P Shipman, Russell russ@sron.nl Pipeline Processing of Stratospheric Terahertz Observatory (STO-2) Galactic Plane Survey
    ABS: The Stratospheric Terahertz Observatory is a balloon born experiment which observed 2.5 deg2 of the Galactic Plane in the [CII] 158 micron transition.  We describe the STO-2 data processing and data products as well as the challenges present with these data.
 
P Shirasaki, Yuji yuji.shirasaki@nao.ac.jp VO service in Japan : Registry service based on Apache Solr and SIA v2 service for Japanese Facilities
    ABS: Currently more than 20 thousands of VO services are registered in the
VO registry database.
Keyword Search is the most popular way to find a resource.
There can be a lot of ways to implement this capability, and the performance
depends on how to index the document describing the resource metadata.
Apache Solr is an open source search platform and uses the Lucene Java
search library for full-text indexing.
The document is indexed after the process that removes stop words
(not adequate for a keyword e.g. a, the, and, is, are ... ) and stems
a word to a root word (e.g. clustering to cluster).
Thus for the query of a keyword cluster, the Solr search system will
return documents that contain a word cluster, clustering, or clustered.
So it increases the probability for a given keyword to hit a desired
resource metadata.

In order to incorporate this feature, we upgraded the registry service
behind the JVO portal to the one based on Solr from XML based DB.
The data from Nobeyama Legacy project were released from the JVO portal
on 1st June in 2018.
The data are now distributed also through the most recent VO standard interface
called SIA-v2.
The data of ALMA and Subaru telescopes are also accessible through the SIA-v2.
We are now working on distributing the data of Hyper-SuprimeCam SSP/DR1 and 
also the data of Hitomi satellite under the collaboration with C-SODA/JAXA.

We present those our recent development on Japanese Virtual Observatory system.
 
P Sihlangu, Isaac isihlangu@ska.ac.za MeerKAT Radio Frequency Interefence characterization using Machine Learning
    ABS: The Square Kilometer Array (SKA) project is an international effort to build the worlds largest radio telescope, with a square kilometre (one million square metres) of collecting area spread over sites in Southern Africa and Australia. It will be the largest and the most sensitive radio telescope ever built and will observe the sky with unprecedented depth and detail. Science results from the SKA telescope are expected to change our understanding of the Universe.

Detection and mitigation of Radio Frequency Interference (RFI) is one of the biggest challenges when analysing the observational data collected from the modern radio telescopes. Due to industrial and technological developments, radio astronomy observations are being
threatened by increasing levels of RFI. Also, an important by−product of the increased collecting area of the these telescopes is their increased sensitivity and as the sensitivity of the these instruments increases, so does the sensitivity to unwanted signals (i.e satellites, mobile cellphones, airplanes, etc). Furthermore the dawn of big data has begun with next generation telescopes such as the SKA that will generate orders of magnitude more data than the current generation. The combination of these issues means that old techniques used to detect and mitigate RFI signals are no longer sufficient. More sophisticated and automated techniques will be required.

My research is focused on finding novel techniques and methods that can provide additional insight into various types and sources of RFI phenomena detected by radio telescopes, specifically via the techniques of machine learning and deep learning. My work aims to characterise and classify RFI signals detected by a brute-force algorithm. I will study and catalogue the different kinds of RFI signals that are detected. Attributes found to be statistically discriminative will be extracted from RFI signals and will be used as feature sets. The attributes extracted will be used to train machine learning algorithms such as Random Forests, Decision Trees and Neural Networks that will classify different types of RFI categories. Thus,
this analysis will help in separating outlier signals into different kinds of RFI or signals of astronomical importance. We will also be able to predict when certain kinds of RFI are likely to occur which will aid in proper scheduling of observations.
 
O Smith, Jeffrey jeffrey.smith@nasa.gov Lilith: A Versatile Instrument and All-Sky Simulator for use with Space-Based Astrophysics Observatories
    ABS: To help facilitate the development of the Transiting Exoplanet Survey Satalite (TESS) data analysis pipeline, it was necessary to produce simulated flight data with sufficient fidelity and volume to exercise all the capabilities of the pipeline in an integrated way. As a generator of simulated flight data, Lilith, was developed for this purpose. We describe the capabilities of the Lilith software package, with particular attention to the interaction between the implemented features and the pipeline capabilities that it exercises.  Using a physics-based TESS instrument and sky model, Lilith creates a set of raw TESS data which includes models for the CCDs, readout electronics, camera optics, behavior of the attitude control system (ACS), spacecraft orbit, spacecraft jitter and the sky, including zodiacal light, and the TESS Input Catalog. The model also incorporates realistic instances of stellar astrophysics, including stellar variability, eclipsing binaries, background eclipsing binaries, transiting planets and diffuse light. This simulated data is then processed through the TESS pipeline generating full archivable data products. Full instrumental and astrophysics ground truth is available and can be used as a training set for TESS data analysis software, such as when training a machine learning classifier for planet candidates. Our intention is to continue to tune Lilith as real TESS flight data becomes available, allowing for an up-to-date simulated set of data products to complement the mission flight data products, thereby aiding researchers as they continue to adapt their tools to the TESS data streams. We discuss the execution performance of the resulting package, and offer some suggestions for improvements for instrument and sky simulators to be developed for other missions.
 
O Snyder, Gregory gsnyder@stsci.edu Mock Datasets and Galaxy Merger Statistics from Cosmological Hydro Simulations
    ABS: I will describe efforts to blend cosmological simulations with surveys of distant galaxies. In particular, I will discuss our work to create and interpret millions of synthetic images derived from the Illustris project, a recent large hydrodynamic simulation effort. Recently, we showed that because galaxies assembled so rapidly, distant mergers are more common than the simplest arguments imply. Further, we improved image-based merger diagnostics by training many-dimensional ensemble learning classifiers using the simulated images and known merger events. By applying these results to data from the CANDELS multi-cycle treasury program, we measured a high galaxy merger rate in the early universe in broad agreement with theory, an important test of our cosmological understanding.
 
X Song, Xinying songxy@ihep.ac.cn Development on Data Analysis Software on GECAM
    ABS: Introduction on Development on Data Analysis Software on GECAM.
 
O Stevens, Abigail abigailstev@gmail.com Stingray: Open-source spectral-timing software
    ABS: New ideas about how to analyze time-domain X-ray astronomy data have initiated the “spectral-timing revolution,” leading to a surge in developments of analysis techniques. Many individual tools and libraries exist, and some are even publicly available, but what has been lacking is a coherent set for a complete analysis. Stingray is a new community-developed, open-source software package in Python for spectral-timing analysis of astrophysical data. This software package merges existing efforts for a timing package in Python and provides the basis for developing spectral-timing analysis tools, while following the Astropy guidelines for modern open-source scientific programming. Stingray has a scripting interface, an affiliated GUI, and a well-documented API for power-users. The ultimate goal is to provide the community with a package that eases the learning curve for state-of-the-art spectral-timing techniques, with a correct statistical framework, to make maximal use of data from NuSTAR, NICER, and potentially STROBE-X and eXTP. Stingray is pip-installable via the Python Package Index, and we warmly welcome community involvement on GitHub and Slack. For more information, see the Stingray website: http://stingraysoftware.github.io/
 
X Stoehr, Felix felix.stoehr@gmx.de Astronomical archives: Serving up the Universe
    ABS: This talk first briefly reviews some of the current context of storing and making astronomical data discoverable and available. We then discuss the challenges ahead and look at the future data-landscape when the next generation of large telescopes will be online, at the next frontier in science archives where also the content of the observations will be described, at the role machine-learning can play as well as at some general aspects of the user-experience for astronomers.
 
P Streicher, Ole ole@aip.de DOI in Astronomy
    ABS: The use of DOI for astronomical data sets is becoming part of good scientific practice. Although a fairly small set of metadata, almost all aspects of findability are covered. The advantage is clearly, that this is no astronomy special but a widely accepted means catering to the quest of interoperability across disciplines, albeit on a basic level. DOI also move the onus of responsibility for proper data sets from the individual scientist to the institutions. Even the IVOA starts to look for ways to incorporate DOI for datasets into their registry and other services.
Still much has to be done: the agreement on curation procedures and what would be
considered as a data set to stam DOI on. At AIP we provide DOI for a range of datasets, and th poster will provide some insight in the curation procedures and decisions about what we consider a data set.
 
P Sutrisno, Raymond rasutrisno@uh.edu Photometric Redshift Estimation: An Active Learning Approach
    ABS: Abstract : A long-lasting problem in astronomy is the accurate
estimation of galaxy distances based solely on the information
contained in photometric filters. Due to observational selection
effects, the spectroscopic (source) sample lacks coverage through-
out the feature space (e.g. colors and magnitudes) compared to
the photometric (target) sample; this results in a clear mismatch
in terms of photometric measurement distributions. We propose
a solution to this problem based on active learning, a machine
learning technique where a sampling strategy enables us to
select the most informative instances to build a predictive model;
specifically, we use active learning following a Query by Committee
approach. We show that by making wisely selected queries in the
target domain we are able to increase our predictive performance
significantly. We show how a relatively small number of queries
(spectroscopic follow-up measurements) suffices to significantly
improve the performance of photometric redshift estimators.
 
O Swade, Daryl swade@stsci.edu The TESS Science Data Archive
    ABS: The Transiting Exoplanet Survey Satellite (TESS) is a survey mission designed to discover exoplanets around the nearest and brightest stars.  TESS is a NASA Astrophysics Explorer mission that was launched on April 18, 2018.  The Mikulski Archive for Space Telescopes (MAST) at the Space Telescope Science Institute serves as the archive for TESS science data.

TESS will conduct large area surveys of bright stars and known M dwarfs within about 60 parsecs.  TESS will observe a single 24 degree by 96-degree sector of the sky for approximately 27 days before pointing to a new sector, surveying almost the entire sky over the two-year prime mission.

Science data are captured in two observation types: Target Data and Full Frame Images.  Target Data result from a subarray of pixels read out with a two-minute cadence from approximately 15,000 stars per sector.  Target stars change every sector, but targets near the ecliptic poles will be observed in multiple sectors.  The entire field of view is captured in Full Frame Images taken at thirty-minute cadence.  Full Frame Images provide a rich source of space-quality continuous light curves for many astronomical investigations.

Archive data products originate from multiple elements within the TESS ground segment.  The Payload Operations Center provides the archive with raw data from the spacecraft, operational files, and focal plane characterization models.  The Science Processing Operations Center pipeline generates FITS files for the Target Data and Full Frame Images.  Light Curves and Centroids are extracted from the Target Data.  Threshold Crossing Events are identified from the Light Curves in the Transiting Planet Search pipeline and Data Validation Reports are generated.  MAST will stage catalog data generated by the TESS Science Office.  The TESS Input Catalog at present contains approximately half a billion persistent luminous objects over the entire sky that are potential two-minute targets or are needed to document nearby fainter stars that contaminate the target photometry.  The TESS Objects of Interest Catalog lists planetary candidates identified as Threshold Crossing Events.  In addition, the TESS Science Office will manage the TESS Follow-up Observing Program in which ground and space based telescopes will be used for further observations of TESS Objects of Interest.  Follow-up Observing Program participants will be responsible for submission of follow-up data to MAST. 

The services provided by MAST for the TESS mission are to store science data and provide an Archive User Interface for data documentation, search, and retrieval.   Target Data and Full Frame Images are expected to be available in MAST within two months after conclusion of a sector’s data collection.  Current estimates predict 30 Terabytes of TESS data available through MAST for each year of the mission.

The MAST architecture is designed to support multiple missions.  MAST currently serves data from the Hubble Space Telescope, Kepler, and approximately ten other missions, as well as numerous additional high-level science data products. MAST will be the JWST data archive.  The TESS mission takes advantage of this multi-mission architecture to provide a cost effective archive that allows integration of TESS data with data from other missions.
 
O Tao, Yihan y.tao@nao.cas.cn Spectral Classification of Galaxies using Deep Neural Networks and Self-taught Learning
    ABS: Spectral classification using machine learning methods is highly in need these days for large sky survey (e.g. LAMOST and SDSS) . These survey produce large amount of spectra data，automated methods can save human researchers from the painful task of classifying and checking the spectra by eye. Recent years, many research has found it promising by using deep neural networks. However, it has the problem of difficult to understand the model, and the trained model is hardly reusable for a new data set. 

In previous work, we carried out experiments using supervised machine learning methods (Logistic Regression, Random Forest and Linear SVM) to select active galactic nucleus (AGNs) from a dataset of galaxy spectra (including AGNs, starburst galaxies, starforming galaxies, and others) provided by SDSS DR14. The results show that the accuracy of these traditional machine learning method is acceptable as the classification accuracy reaches ~91%. However, it can be limited because we used linear PCA of 20 components to present spectra features.

In this paper, we investigate the accuracy of applying deep neural networks (DNNs) to learn the feature representation from the galaxy spectra. We trained a DNN of three hidden layer and its test accuracy achieved higher than 95%. To make the model reusable and more interpretable, we also employed self-taught learning (unsupervised feature learning) to let the spectra data speak for itself. The advantage of self-taught learning and unsupervised feature learning is that we can get the algorithms to learn a feature representation from large amount of unlabeled data. This is especially useful for some kind of galaxies (such as LINERs, BL Lacs) which we have relatively small amount of labeled spectra. We trained a 7-layer auto-encoder (2600-1024-256-20-256-768-1024-2600) and then feed into the classifiers.  In addition, we present and visualize the features learnt from our auto-encoder and try to explain which part of the spectra is more important for classification.
 
O Tavagnacco, Daniele daniele.tavagnacco@inaf.it Performance-related aspects in the Big Data Astronomy Era: architects in software optimization
    ABS: In the last decades the amount of data collected by Astronomical Instruments and the evolution of computational demands have grown exponentially. Today it is not possible to obtain scientific
results without prodigious amounts of computation. For this reason, the software performance plays a key role in modern Astronomy data analysis.
Scientists tend to write code with the only goal of implementing the algorithm in order to achieve a solution. Code modifications to gain better performance always come later. However, as computing
architectures evolve to match the performance that is demanded, the coding task has to encompass the exploitation of the architecture design, the single-processor performance and parallelization.
To facilitate this task, programming languages are progressing and introducing new features to fully make use of the hardware architecture. Designing a software that meets performance, memory
efficiency, maintainability, and scalability requirements is a complex task that should be addressed by the software architect. The complexity stems from the existence of multiple alternative solutions
for the same requirements, which make tradeoffs inevitable.
In this contribution we will present part of the activity done at the Italian Science Data Center for the ESA’s cosmological space mission Euclid which regards the software performance optimization.
In particular, considering the programming languages selected for the development of the Euclid scientific pipelines, we will present some C++ and Python examples focusing on the main aspects of human contribution in the optimization of the code from the performance, memory efficiency and maintainability point of view.
 
P Teuben, Peter teuben@astro.umd.edu QAC: Quick Array Combinations with CASA
    ABS: A simple python layer in CASA was developed to aid in writing scripts for array (single dish and interferometric) combination. Although initially developed for TP2VIS, running simulations and comparing with other array combination methods, and adding regressions became cumbersome, and QAC was developed. Both ALMA, ngVLA and CARMA simulations are already supported, but extending to more generic array are planned.
 
O Toledo, Ignacio ignacio.toledo@alma.cl Data Science =! Software Engineering. Exploring a workflow for ALMA operations.
    ABS: In the last few years Data science has emerged as a discipline of its own to address problems where data is usually heterogeneous, complex and abundant. In a nutshell, data science allows to provide answers to situations where a hypothesis can be formulated and later can be either confirmed or rejected following standard scientific methodology using data as raw material. Data science has been called differently depending of the domain (business intelligence, operational management, astroinformatics) and it has been recently in the center of a hype related to artificial intelligence and machine learning. It has been quickly adopted by the digital industry as the tool to distill information of massive operational data sets.

Among the many tools data science requires (mathematics, statistics, domain knowledge of the data sets, …), IT infrastructure and software is by far the most visible and there is at present a whole ecosystem available as open source projects. The downside of this is data science is commonly confused with IT and software development, which creates conflicts between engineering- and scientific- mindsets, and leads to wrongly applying software development methodologies to it neglecting the experimental nature of the problem. In summary, creating the data lab becomes more important than answering questions with it.

In the domain of ALMA operations, there are many instances that can be identified and described as data science cases or projects ranging from monitoring array elements to understand performances and predict faults for engineering operations to routine monitoring of calibrators for science operations purposes. We have identified already around 30 different initial questions (or data science cases) and found that several of them have been addressed through individual efforts. 

In parallel, several enabling platforms or frameworks have appear in the ecosystem that provides data scientists with both the “laboratory equipment” to conduct their “experiments” as well as enabling tools for collaboration, versioning control, and deploying results in production with a quick turnaround.

This talk aims to summarize the results of our exploration to apply data science workflows to resolve ALMA operations issues, identify suitable platforms that are already in use by the industry, share our experience in addressing specific ALMA operations data cases, and discuss the technical and sociological challenges we encountered along the way.​​
 
O Tollerud, Erik etollerud@stsci.edu BoF: Open Source/Development Software Projects and Large Organizations/Missions: Recommendations and Challenges
    ABS: Independent open-develped projects have been growing rapidly in astronomy and related domains - e.g., projects like Astropy, Sunpy, or Scipy. Large astronomy organizations or missions regularly make use of the software products of these projects, and at least in some cases, contribute back. This has the obvious benefit of being able to do more for the scientific community with less effort per mission.  However, there are some challenges in this process - inflexible or agressive deadlines by funded organizations may conflict with the timelines of open source projects, science vs engineering cultural conflicts may make contribution more difficult, mismatch between the needs of the general community and a specific mission, etc.  This BoF session is aimed at discussing exactly these tensions (or others that are brought up in the discussion).  The goal is to draft a set of suggestions that interested parties can take to mission/institution leadership to help resolve these tensions, or at least be able to enter into them with eyes open.
 
O Tomasi, Maurizio maurizio.tomasi@unimi.it Towards new solutions for scientific computing: the case of Julia
    ABS: This year marks the consolidation of Julia (https://julialang.org/), a programming language designed for scientific computing, as the last version before 1.0 has just been released (0.7). Among its main features, expressiveness and high execution speeds are the most prominent: the performance of Julia code is similar to statically compiled languages, yet Julia provides a nice interactive shell and fully supports Jupyter; moreover, it can transparently call external codes written in C, Fortran, and even Python without the need of wrappers. The usage of Julia in the astronomical community is growing, and a GitHub organization named JuliaAstro takes care of coordinating the development of packages. In this talk, we will provide an overview of Julia and JuliaAstro. We will also provide a real-life example by discussing the implementation of a Julia-only simulation pipeline for a large-scale CMB experiment.
 
O Vitello, Fabio fabio.vitello@inaf.it VisIVO Visual Analytics Tool an EOSC Science Demonstrator for data discovery
    ABS: VisIVO is an integrated suite of tools and services for data discovery that include collaborative portals, mobile applications, visual analytics tool and a number of key components such as workflow applications, analysis and data mining functionalities.

Space missions and ground-based facilities produce massive volumes of data and the ability to collect and store them is increasing at a higher pace than the ability to analyze them. This gap leads to new challenges in the analysis pipeline to discover information contained in the data. 

VisIVO Visual analytics tool for star formation regions focuses on handling these massive and heterogeneous volumes of information accessing the data previously processed by data mining algorithms and advanced analysis techniques with highly interactive visual interfaces offering scientists the opportunity for in-depth understanding of massive, noisy, and high-dimensional data.

The aforementioned challenges demands an increasing archiving and computing resources as well as a federated and interoperable virtual environment enabling collaboration and re-use of data and knowledge. Thus, the connection with the European Open Science Cloud is being investigated exploiting the EGI services such as the Check-in (for federated authentication and authorization) and the Federated Cloud (for analysis and archiving services). 

Recently the VisIVO development has been exploited for the experimentation of cutting-edge interactive visualization technologies for the improvement of teaching and scientific dissemination. This work is being carried out as a knowledge transfer from astrophysical sciences to geological sciences in the context of an international collaboration to innovate teaching, learning and dissemination of earth sciences, using virtual reality.
 
P Wang, Rui wangrui@nao.cas.cn Analysis of Stellar Spectra from LAMOST DR5 with Generative Spectrum Networks
    ABS: We derived the fundamental stellar atmospheric parameters (Teff, log g, [Fe/H] and [α/Fe]) of low-resolution spectroscopy from LAMOST DR5 with Generative Spectrum Networks(GSN), which follows the same scheme as a normal ANN with stellar parameters as the inputs and spectrum as outputs. After training on PHOENIX theoretical spectra, the GSN model performed effectively on producing synthetic spectra. Combining with Bayes framework,  application in analysis of LAMOST observed spectra become efficient on the Spark platform. Also, we examined and validated the results by comparing with reference parameters of high-resolution surveys and asteroseismic results. Our method is credible with a precision of ~130K for Teff, ~0.15 dex for log g, ~0.13 dex for [Fe/H] and ~0.10 dex for [α/Fe].
 
P Wicenec, Andreas andreas.wicenec@uwa.edu.au The Murchison Widefield Array's VO compliant archive
    ABS: The Murchison Widefield Array is a SKA pre-cursor operated on the site of the planned SKA1-LOW. It  has been operational since more than 4 years and during that time it has collected around 25 PB of data. During 2018 we have been working on a separately funded project to implement a completely new access system to the MWA archive, which also includes an on-the-fly calibration pipeline and is based on IVOA standards. As part of the implementation we have also implemented a new VOSpace server system in Python, which has been released as an open source contribution to the community. In this paper we are presenting an overview of the new MWA archive system, including the web interface, authentication/authorisation, data processing, staging and delivery.
 
X Wise, Michael wise@astron.nl Establishing the SKA Regional Centre Network: Mesh Management and Culture Change
    ABS: The Square Kilometre Array (SKA) is an ambitious project to construct the world’s most powerful radio telescope and enable transformational scientific discoveries across a wide range of topics in physics and astronomy. With two telescopes sites located in the deserts of South African and West Australia, an operational headquarters based in the UK, and 12 different member countries contributing to the design and construction, the SKA is truly a global endeavor. Once operational, the SKA is expected to produce an archive of science data products with an impressive growth rate on the order of 700 petabytes per year. Hosting the resulting SKA archive and subsequent science extraction by users will require a global research infrastructure providing additional capacity in networking, storage, computing, and support. 

This research infrastructure is currently foreseen to take the form of a federated, global network of SKA Regional Centres (SRCs). These SRCs will be the primary interface for researchers in extracting scientific results from SKA data and, as such, are essential to the ultimate success of the telescope. The unprecedented scale of the expected SKA data stream, however, requires a fundamental change in the way radio astronomers approach extracting their science. Efforts are already underway in various countries around the world to define and deploy the seeds of what will grow into a community-provided research infrastructure that can deliver SKA science. In this talk, I will give an update on these initial efforts as well as the various technological, management, and sociological challenges associated with establishing the SKA Regional Centre network.
 
P Woods, Paul paul.woods@nature.com 
    ABS: 
 
P Xiao, Jian xiaojian@tju.edu.cn Research on Automatic Recognition of Radio Frequency Interference Based on Deep Learning
    ABS: Radio frequency interference (RFI) mitigation is a key phase in data pipeline of radio telescopes. Classical RFI mitigation methods depend on the RFI physical characteristics, often fail to recognize some complicated patterns or result in misrecognition. We developed a novel approach of RFI recognition and automatic flagging with an improved convolution neural network. The network was constructed based on U-net with much deeper network structure for more complicated patterns and more components to reduce recognition error caused by over-fitting. Compared to the most widely used classical method -- SumThreshold, our method has better performance on both precision and recall rate, it also outperformed the U-net, a traditional deep learning model, and KNN,a typical machine learning model.
 
P Xu, Yang yxu@nao.cas.cn An algorithm of selection of meteor candidates in GWAC system
    ABS: With its large field of view, GWAC can record hundreds of meteors every day. These meteors disturb the detection process of optical transients in the GWAC system, and may be a valuable treasure for some meteor research groups. It is therefore very important to accurately find all of these meteors. To address the challenge of precisely distinguishing meteors from other elongated objects in a GWAC-like sky survey system, we design and implement a meteor candidate recognition algorithm, including the recognizing and morphology analysis of the light curves of the meteor candidates. Although the algorithm may filter out some real meteors, it can provide a sample of meteor with high confidence. After processing the images of Mini-GWAC taken in two months, we detect 109,000 elongated objects in which more than 90 percent of objects are not meteor. Among the elongated objects, about 5.9% objects are identified as meteors with high confidence, after the filters based upon an existence in a single frame, a single peak in the light curves, and a slow variation of the light curves.
 
P Yunfei, Xu xuyf@nao.cas.cn 
    ABS: 
 
X Zapart, Christopher chris.zapart@nao.ac.jp An introduction to FITSWebQL
    ABS: The JVO ALMA WebQL web service - available through the JVO ALMA FITS archive - has been upgraded to include legacy data from other telescopes, for example Nobeyama NRO45M in Japan. The updated server software has been renamed FITSWebQL. In addition, a standalone desktop version supporting Linux, macOS and Windows 10 Linux Subsystem (Bash on Windows) is also available for download from http://jvo.nao.ac.jp/~chris/ .

The FITSWebQL server enables viewing of even 100GB-large FITS files in a web browser running on a PC with a limited amount of RAM. Users can interactively zoom-in to selected areas of interest with the corresponding frequency spectrum being calculated on the server in near real-time. The client (a browser) is a JavaScript application built on AJAX, WebSockets, HTML5, WebGL and SVG.

There are many challenges when providing a web browser-based real-time FITS data cube preview service over high-latency low-bandwidth network connections. The upgraded version tries to overcome the latency issue by predicting user mouse movements with a Kalman Filter in order to speculatively deliver the real-time spectrum data at a point where the user is likely to be looking at. The new version also allows one to view multiple FITS files simultaneously in an RGB composite mode (NRO45M FUGIN only), where each dataset is assigned one RGB channel to form a colour image. Spectra from multiple FITS cubes are shown together too.

The talk gives a brief tour of the FITSWebQL main features. We also touch on some of the recent developments, such as an experimental switch from C/C++ to Rust (see https://www.rust-lang.org/) for improved stability, better memory management and fearless concurrency, or attempts to display FITS data cubes in the form of interactive on-demand video streams in a web browser.
 
O Zecevic, Petar Petar.Zecevic@fer.hr AXS: Making end-user petascale analyses possible, scalable, and usable
    ABS: We introduce AXS (Astronomy eXtensions for Spark), a scalable open-source astronomical data analysis framework built on Apache Spark, a state-of-the-art industry-standard engine for big data processing.

In the age when the most challenging questions of the day demand repeated, complex processing of large information-rich tabular datasets, scalable and stable tools that are easy to use by domain practitioners are crucial. Building on capabilities present in Spark, AXS enables querying and analyzing almost arbitrarily large astronomical catalogs using familiar Python/AstroPy concepts, DataFrame APIs, and SQL statements. AXS supports complex analysis workflows with astronomy-specific operations such as spatial selection or on-line cross-matching. Special attention has been given to usability, from conda packaging to enabling ready-to-use cloud deployments.

AXS is regularly used within the University of Washington's DIRAC Institute, enabling the analysis of ZTF (Zwicky Transient Facility) and other datasets. As an example, AXS is able to cross-match Gaia DR2 (1.8 billion rows) and SDSS (800 million rows) in 2 minutes, with the data of interest (photometry) being passed to Python routines for further processing. Here, we will present current AXS capabilities, give an overview of future plans, and discuss some implications to analysis of LSST and similarly sized datasets. The long-term goal of AXS is to enable petascale catalog and stream analyses by individual researchers and groups.
 
P Zhang, Yanxia zyx@bao.ac.cn Machine Learning for Quasar Candidate Selection
    ABS: With the big data era of astronomy coming, machine learning becomes more popular in various astronomical aspects, for example, classification of celestial objects, physical parameter measurement, rare object detection and so on. Quasar candidate selection belongs to the classification problem. Based on a number of present photometric and spectroscopic sky survey projects, preselecting quasar candidates from galaxies and stars is a hot issue. It is easy to separate quasars and stars from galaxies by morphology while it is difficult to distinguish quasars from stars due to their similar morphology. Nevertheless, researchers have figured out various approaches on quasar candidate selection. We will introduce them in detail.
